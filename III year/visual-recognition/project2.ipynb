{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w87frMXHNvpu"
      },
      "source": [
        "# ProtoNet and Visual Meta-Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BLoGVNSNvpw"
      },
      "source": [
        "In this task, you are asked to implement a model which can quickly adapt to new classes and/or tasks with few samples. We will build the architecture inspired from the work: Prototypical Networks ([Snell et al., 2017](https://arxiv.org/pdf/1703.05175.pdf))\n",
        "\n",
        "* We will focus on the task of few-shot classification where the training and test set have distinct sets of classes.\n",
        "\n",
        "* You will apply ProtoNet to the CIFAR100 and then test its performance on out-of-distribution data in the SVHN dataset.\n",
        "\n",
        "\n",
        "The task is divided into four parts that contribute to your total score as follows:\n",
        "* Dataset Preparation = 1p\n",
        "* Few-Shot Sampler = 3p\n",
        "* Prototypical Networks + Advanced Techniques = (1 + 1 + 2 + 1) = 5p\n",
        "* Domain adaptation in the SVHN experiment = 1p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eQXWWFBPu36"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFoYYys7Nvpw",
        "outputId": "83e1ecf9-3819-4244-e36c-4d89c1dfa297"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from statistics import mean, stdev\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR100, SVHN, MNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"./data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"./saved_models\"\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(CHECKPOINT_PATH):\n",
        "    os.makedirs(CHECKPOINT_PATH)\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H4Zm8QmNvpx"
      },
      "source": [
        "# Dataset Preparation (1p)\n",
        "\n",
        "CIFAR100 has 100 classes and images of size $32\\times 32$ pixels. Instead of splitting the training, validation, and test set over examples, we will split them over classes: we will use 80 classes for training, and 10 for validation, and 10 for testing. Our overall goal is to obtain a model that can distinguish between the 10 test classes while seeing very few examples. First, let's load the dataset and visualize some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "gmUIdE8nNvpy",
        "outputId": "d9c84bef-8acb-4b83-e13b-f06269b894c8"
      },
      "outputs": [],
      "source": [
        "# Load the CIFAR dataset\n",
        "CIFAR_train_set = CIFAR100(root=DATASET_PATH, train=True, download=True, transform=transforms.ToTensor())\n",
        "CIFAR_test_set = CIFAR100(root=DATASET_PATH, train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# TODO: Visualise some images in a grid\n",
        "def show_images(images, num_images=16, images_per_row=4):\n",
        "    num_rows = (num_images + images_per_row - 1) // images_per_row\n",
        "    plt.figure(figsize=(4, 4))\n",
        "\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        if isinstance(images, torch.utils.data.Dataset):\n",
        "            image, _ = images[i]\n",
        "        else:\n",
        "            image = images[i]\n",
        "\n",
        "        image = image.numpy()\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "        plt.subplot(num_rows, images_per_row, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_images(CIFAR_train_set, num_images=16, images_per_row=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqti6EZ9Qtw1"
      },
      "source": [
        "Prepare the dataset in the training, validation and test split as mentioned before. The torchvision package gives us the training and test set as two separate dataset objects. Merge the original training and test set, and then create the new train-val-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIh6ez1NNvpy"
      },
      "outputs": [],
      "source": [
        "# Merging original training and test set\n",
        "CIFAR_all_images = np.concatenate([CIFAR_train_set.data, CIFAR_test_set.data], axis=0)\n",
        "CIFAR_all_targets = torch.LongTensor(CIFAR_train_set.targets + CIFAR_test_set.targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1g9CdNqNvpy"
      },
      "source": [
        "Define our own, dataset class below.\n",
        "It needs to:\n",
        "\n",
        "- Take a set of images, labels/targets, and image transformations\n",
        "- Return the corresponding images and labels element-wise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OGZBp-CNvpz"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, imgs, targets, img_transform=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            imgs - Numpy array of shape [N,32,32,3] containing all images.\n",
        "            targets - PyTorch array of shape [N] containing all labels.\n",
        "            img_transform - A torchvision transformation that should be applied\n",
        "                            to the images before returning. If none, no transformation\n",
        "                            is applied.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.img_transform = img_transform\n",
        "        self.imgs = imgs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO: Fill this\n",
        "        img = self.imgs[idx]\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        if self.img_transform is not None:\n",
        "            img = self.img_transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: Fill this\n",
        "        return len(self.imgs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_e8hDifNvpz"
      },
      "source": [
        "Create the class splits. Assign the classes randomly to training, validation and test, and use a 80%-10%-10% split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM9oq8X5Nvpz"
      },
      "outputs": [],
      "source": [
        "classes = torch.randperm(100)  # Returns random permutation of numbers 0 to 99\n",
        "train_classes, val_classes, test_classes = classes[:80], classes[80:90], classes[90:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g63CdR7JNvpz"
      },
      "source": [
        "Classes have quite some variety and some classes might be easier to distinguish than others.\n",
        "\n",
        "We want to learn the classification of those ten classes from 80 other classes in our training set, and few examples from the actual test classes.\n",
        "\n",
        "\n",
        "You need to experiment with different number of examples per class.\n",
        "\n",
        "Create the training, validation and test dataset according to our split above. For this, we create dataset objects of our previously defined class `ImageDataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTFKPTLANvpz"
      },
      "outputs": [],
      "source": [
        "def dataset_from_labels(imgs, targets, class_set, img_transform):\n",
        "    # TODO: Return an ImageDataset object representing a train / val / test set.\n",
        "    # Its should use the set of all CIFAR images, targets and class split calculated above with the 80-10-10 rule.\n",
        "    class_set = list(set(class_set))\n",
        "\n",
        "    indices = [i for i, t in enumerate(targets) if int(t) in class_set]\n",
        "\n",
        "    filtered_imgs = imgs[indices]\n",
        "    filtered_targets = targets[indices]\n",
        "\n",
        "    return ImageDataset(filtered_imgs, filtered_targets, img_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HE2YbV8Nvpz"
      },
      "outputs": [],
      "source": [
        "# Pre-computed statistics from the new train set\n",
        "DATA_MEANS = torch.Tensor([0.5183975 , 0.49192241, 0.44651328])\n",
        "DATA_STD = torch.Tensor([0.26770132, 0.25828985, 0.27961241])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(\n",
        "                                         DATA_MEANS, DATA_STD)\n",
        "                                     ])\n",
        "# For training, try adding some augmentations as well.\n",
        "train_transform = transforms.Compose([ # TODO: Fill This #\n",
        "                                      transforms.ToPILImage(),\n",
        "                                      transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                      transforms.RandomRotation(15),\n",
        "                                      transforms.RandomGrayscale(p=0.1),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(DATA_MEANS, DATA_STD),\n",
        "                                      ])\n",
        "\n",
        "train_set = dataset_from_labels(\n",
        "    CIFAR_all_images, CIFAR_all_targets, train_classes, img_transform=train_transform)\n",
        "val_set = dataset_from_labels(\n",
        "    CIFAR_all_images, CIFAR_all_targets, val_classes, img_transform=test_transform)\n",
        "test_set = dataset_from_labels(\n",
        "    CIFAR_all_images, CIFAR_all_targets, test_classes, img_transform=test_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC1A4G-LNvp0"
      },
      "source": [
        "# Few-Shot Sampler (3p)\n",
        "\n",
        "## The Core Concept\n",
        "\n",
        "Prototypical Networks simulate few-shot learning during training by:\n",
        "\n",
        "1. Randomly selecting N classes (N-way classification)\n",
        "2. Sampling K examples per class for the **support set** (K-shot learning)\n",
        "3. Sampling additional examples from the same classes for the **query set**\n",
        "\n",
        "The model learns to classify query examples by comparing them to class prototypes computed from the support set.\n",
        "\n",
        "## Implementation Task\n",
        "\n",
        "Complete the `FewShotBatchSampler` class to:\n",
        "- Select random classes for each episode\n",
        "- Create support and query sets from these classes\n",
        "- Ensure proper indexing for the dataloader\n",
        "\n",
        "This sampler will enable the N-way, K-shot training regime needed for few-shot learning.\n",
        "\n",
        "**Hint:** Refer to PyTorch's [Sampler documentation](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler) for implementation details. You'll create a custom sampler that controls which data examples are used in each training batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUOgetb3Nvp0"
      },
      "outputs": [],
      "source": [
        "class FewShotBatchSampler(object):\n",
        "\n",
        "    def __init__(self, dataset_targets, N_way, K_shot, include_query=False, shuffle=True, shuffle_once=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            dataset_targets - PyTorch tensor of the labels from the dataset in the order they occur in it.\n",
        "            N_way - Number of classes to sample per batch.\n",
        "            K_shot - Number of examples to sample per class in the batch.\n",
        "            include_query - If True, returns batch of size N_way*K_shot*2, which\n",
        "                            can be split into support and query set. Simplifies\n",
        "                            the implementation of sampling the same classes but\n",
        "                            distinct examples for support and query set.\n",
        "            shuffle - If True, examples and classes are newly shuffled in each\n",
        "                      iteration (for training)\n",
        "            shuffle_once - If True, examples and classes are shuffled once in\n",
        "                           the beginning, but kept constant across iterations\n",
        "                           (for validation)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dataset_targets = dataset_targets\n",
        "        self.N_way = N_way\n",
        "        self.K_shot = K_shot\n",
        "        self.shuffle = shuffle\n",
        "        self.include_query = include_query\n",
        "        if self.include_query:\n",
        "            self.K_shot *= 2\n",
        "        self.batch_size = self.N_way * self.K_shot  # Number of overall images per batch\n",
        "\n",
        "        # Organize examples by class\n",
        "        self.classes = torch.unique(self.dataset_targets).tolist()\n",
        "        self.num_classes = len(self.classes)\n",
        "        self.indices_per_class = {}\n",
        "        self.batches_per_class = {}  # Number of K-shot batches that each class can provide\n",
        "        for c in self.classes:\n",
        "            self.indices_per_class[c] = torch.where(self.dataset_targets == c)[0]\n",
        "            self.batches_per_class[c] = self.indices_per_class[c].shape[0] // self.K_shot\n",
        "\n",
        "        # Create a list of classes from which we select the N classes per batch\n",
        "        self.iterations = sum(self.batches_per_class.values()) // self.N_way\n",
        "        self.class_list = [c for c in self.classes for _ in range(self.batches_per_class[c])]\n",
        "        if shuffle_once or self.shuffle:\n",
        "            self.shuffle_data()\n",
        "        else:\n",
        "            # For testing, we iterate over classes instead of shuffling them\n",
        "            sort_idxs = [i+p*self.num_classes for i,\n",
        "                         c in enumerate(self.classes) for p in range(self.batches_per_class[c])]\n",
        "            self.class_list = np.array(self.class_list)[np.argsort(sort_idxs)].tolist()\n",
        "\n",
        "    def shuffle_data(self):\n",
        "        for c in self.classes:\n",
        "            perm = torch.randperm(self.indices_per_class[c].shape[0])\n",
        "            self.indices_per_class[c] = self.indices_per_class[c][perm]\n",
        "        random.shuffle(self.class_list)\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Todo: Fill this using the above code and the following directives #\n",
        "        # Step 1) Shuffle data\n",
        "        # Step 2) Sample few-shot batches.\n",
        "        # Step 3) Select N classes for the batch\n",
        "        # Step 4) For each class, select the next K examples and add them to the batch\n",
        "        # Step 5) Take into account the self.include_query variable and return support+query set, if True.\n",
        "\n",
        "        if self.shuffle:\n",
        "            self.shuffle_data()\n",
        "        class_counters = {c: 0 for c in self.classes}\n",
        "\n",
        "        for it in range(self.iterations):\n",
        "            batch = []\n",
        "            support_inds = []\n",
        "            query_inds = []\n",
        "\n",
        "            subset = self.class_list[it * self.N_way:(it + 1) * self.N_way]\n",
        "\n",
        "            for c in subset:\n",
        "                counter = class_counters[c]\n",
        "                start = counter * self.K_shot\n",
        "                end = start + self.K_shot\n",
        "                inds = self.indices_per_class[c][start:end].tolist()\n",
        "\n",
        "                if self.include_query:\n",
        "                    half = self.K_shot // 2\n",
        "                    support_inds.extend(inds[:half])\n",
        "                    query_inds.extend(inds[half:])\n",
        "                else:\n",
        "                    batch.extend(inds)\n",
        "\n",
        "                class_counters[c] += 1\n",
        "\n",
        "            if self.include_query:\n",
        "                batch = support_inds + query_inds\n",
        "\n",
        "            yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWcAtlZENvp0"
      },
      "source": [
        "Now, create our intended data loaders by passing an object of `FewShotBatchSampler` as `batch_sampler=...` input to the PyTorch data loader object.\n",
        "\n",
        "## Configuring Data Loaders\n",
        "\n",
        "Use a 5-class 4-shot training setting:\n",
        "- **N-way**: 5 classes per episode\n",
        "- **K-shot**: 4 examples per class for the support set\n",
        "- **Total support set size**: 20 images (5 × 4)\n",
        "\n",
        "This configuration means each support set contains examples from 5 random classes with 4 examples per class. While it's usually best to match the training shots with your test configuration, we're using 4 as a compromise to allow for experimenting with different shot numbers later.\n",
        "\n",
        "For optimal performance, you could treat the number of training shots as a hyperparameter in a grid search, but 4 shots works well for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V81_J-YNvp0",
        "outputId": "3357bdd3-6d81-407d-bf69-46be2bed98c3"
      },
      "outputs": [],
      "source": [
        "N_WAY = 5\n",
        "K_SHOT = 4\n",
        "train_data_loader = data.DataLoader(train_set,\n",
        "                                    batch_sampler=FewShotBatchSampler(train_set.targets,\n",
        "                                                                      include_query=True,\n",
        "                                                                      N_way=N_WAY,\n",
        "                                                                      K_shot=K_SHOT,\n",
        "                                                                      shuffle=True),\n",
        "                                    num_workers=4)\n",
        "val_data_loader = data.DataLoader(val_set,\n",
        "                                  batch_sampler=FewShotBatchSampler(val_set.targets,\n",
        "                                                                    include_query=True,\n",
        "                                                                    N_way=N_WAY,\n",
        "                                                                    K_shot=K_SHOT,\n",
        "                                                                    shuffle=False,\n",
        "                                                                    shuffle_once=True),\n",
        "                                  num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8eu0l13Nvp0"
      },
      "source": [
        "The sampling of a support and query set should be implemented as sampling method from a support set with twice the number of examples, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w3NLQTlNvp0"
      },
      "outputs": [],
      "source": [
        "def split_batch(imgs, targets):\n",
        "    support_imgs, query_imgs = imgs.chunk(2, dim=0)\n",
        "    support_targets, query_targets = targets.chunk(2, dim=0)\n",
        "    return support_imgs, query_imgs, support_targets, query_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct5lpKjcNvp1"
      },
      "source": [
        "Finally, to ensure that our implementation of the data sampling process is correct, we can sample a batch and visualize its support and query set. What we would like to see is that the support and query set have the same classes, but distinct examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "1Me7AG6cNvp1",
        "outputId": "2d8c121e-914f-47cc-f621-8af2a11751a4"
      },
      "outputs": [],
      "source": [
        "# Todo: Fill this #\n",
        "\n",
        "def denormalize(imgs, mean, std):\n",
        "    return imgs * std + mean\n",
        "\n",
        "batch_imgs, batch_targets = next(iter(train_data_loader))\n",
        "support_imgs, query_imgs, support_targets, query_targets = split_batch(batch_imgs, batch_targets)\n",
        "\n",
        "assert support_imgs.shape[0] == N_WAY * K_SHOT\n",
        "assert query_imgs.shape[0] == N_WAY * K_SHOT\n",
        "assert support_targets.tolist() == query_targets.tolist()\n",
        "\n",
        "mean_data = DATA_MEANS.view(1, 3, 1, 1)\n",
        "std_data = DATA_STD.view(1, 3, 1, 1)\n",
        "support_imgs = denormalize(support_imgs, mean_data, std_data)\n",
        "query_imgs = denormalize(query_imgs, mean_data, std_data)\n",
        "\n",
        "print(\"Support images\")\n",
        "show_images(support_imgs, num_images=20)\n",
        "print(\"\\nQuery images\")\n",
        "show_images(query_imgs, num_images=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYPYXWm4Nvp1"
      },
      "source": [
        "# Prototypical Networks\n",
        "\n",
        "The Prototypical Network, or ProtoNet for short, is a metric-based meta-learning algorithm that operates similarly to the nearest neighbor classification. Metric-based meta-learning methods classify a new example $\\mathbf{x}$ based on some distance function $d_{\\varphi}$ between $x$ and all elements in the support set. ProtoNets implements this idea with the concept of prototypes in a learned feature space. First, ProtoNet uses an embedding function $f_{\\theta}$ to encode each input in the support set into a $L$-dimensional feature vector. Next, for each class $c$, we collect the feature vectors of all examples with label $c$ and average their feature vectors. Formally, we can define this as:\n",
        "\n",
        "$$\\mathbf{v}_c=\\frac{1}{|S_c|}\\sum_{(\\mathbf{x}_i,y_i)\\in S_c}f_{\\theta}(\\mathbf{x}_i)$$\n",
        "\n",
        "where $S_c$ is the part of the support set $S$ for which $y_i=c$, and $\\mathbf{v}_c$ represents the _prototype_ of class $c$. The prototype calculation is visualized below for a 2-dimensional feature space and 3 classes. The colored dots represent encoded support elements with the color-corresponding class labels, and the black dots next to the class label are the averaged prototypes.\n",
        "\n",
        "![protonet_classification.svg](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:xlink="http://www.w3.org/1999/xlink"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="162.27037pt"
   height="98.094475pt"
   viewBox="0 0 162.27037 98.094474"
   version="1.2"
   id="svg1079"
   sodipodi:docname="protonet_classification.svg"
   inkscape:version="1.0.2 (e86c8708, 2021-01-15)">
  <metadata
     id="metadata1083">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title></dc:title>
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <sodipodi:namedview
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1"
     objecttolerance="10"
     gridtolerance="10"
     guidetolerance="10"
     inkscape:pageopacity="0"
     inkscape:pageshadow="2"
     inkscape:window-width="1440"
     inkscape:window-height="761"
     id="namedview1081"
     showgrid="false"
     fit-margin-top="0"
     fit-margin-left="0"
     fit-margin-right="0"
     fit-margin-bottom="0"
     inkscape:zoom="3.20899"
     inkscape:cx="108.52474"
     inkscape:cy="65.74279"
     inkscape:window-x="0"
     inkscape:window-y="25"
     inkscape:window-maximized="0"
     inkscape:current-layer="surface11253" />
  <defs
     id="defs920">
    <g
       id="g881">
      <symbol
         overflow="visible"
         id="glyph0-0">
        <path
           style="stroke:none"
           d=""
           id="path851" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph0-1">
        <path
           style="stroke:none"
           d="M 5.828125,-6.578125 C 5.53125,-6.328125 5.5,-5.984375 5.5,-5.84375 c 0,0.71875 0.5625,1.03125 1,1.03125 0.5,0 1.015625,-0.34375 1.015625,-1.03125 0,-1.390625 -1.875,-1.484375 -2.8125,-1.484375 -2.890625,0 -4.09375,1.859375 -4.09375,3.734375 0,2.15625 1.53125,3.6875 4.015625,3.6875 2.625,0 3.109375,-1.875 3.109375,-1.984375 C 7.734375,-2.125 7.5,-2.125 7.34375,-2.125 c -0.296875,0 -0.3125,0.03125 -0.390625,0.25 C 6.53125,-0.859375 5.75,-0.546875 4.9375,-0.546875 c -2.25,0 -2.25,-2.375 -2.25,-3.125 0,-0.90625 0,-3 2.078125,-3 0.578125,0 0.84375,0.046875 1.0625,0.09375 z m 0,0"
           id="path854" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-0">
        <path
           style="stroke:none"
           d=""
           id="path857" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-1">
        <path
           style="stroke:none"
           d="m 3.796875,-7.203125 c 0,-0.3125 -0.03125,-0.3125 -0.34375,-0.3125 -0.71875,0.703125 -1.75,0.71875 -2.21875,0.71875 v 0.40625 c 0.265625,0 1.015625,0 1.640625,-0.3125 v 5.78125 c 0,0.359375 0,0.515625 -1.125,0.515625 H 1.3125 V 0 c 0.203125,-0.015625 1.59375,-0.046875 2.015625,-0.046875 0.359375,0 1.78125,0.03125 2.03125,0.046875 v -0.40625 h -0.4375 c -1.125,0 -1.125,-0.15625 -1.125,-0.515625 z m 0,0"
           id="path860" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph2-0">
        <path
           style="stroke:none"
           d=""
           id="path863" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph2-1">
        <path
           style="stroke:none"
           d="M 5.828125,-6.578125 C 5.53125,-6.328125 5.5,-5.984375 5.5,-5.84375 c 0,0.71875 0.5625,1.03125 1,1.03125 0.5,0 1.015625,-0.34375 1.015625,-1.03125 0,-1.390625 -1.875,-1.484375 -2.8125,-1.484375 -2.890625,0 -4.09375,1.859375 -4.09375,3.734375 0,2.15625 1.53125,3.6875 4.015625,3.6875 2.625,0 3.109375,-1.875 3.109375,-1.984375 C 7.734375,-2.125 7.5,-2.125 7.34375,-2.125 c -0.296875,0 -0.3125,0.03125 -0.390625,0.25 C 6.53125,-0.859375 5.75,-0.546875 4.9375,-0.546875 c -2.25,0 -2.25,-2.375 -2.25,-3.125 0,-0.90625 0,-3 2.078125,-3 0.578125,0 0.84375,0.046875 1.0625,0.09375 z m 0,0"
           id="path866" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph2-2">
        <path
           style="stroke:none"
           d="m 5.59375,-3.984375 1.890625,-2.09375 c 0.21875,-0.234375 0.3125,-0.34375 1.65625,-0.34375 V -7.1875 C 8.578125,-7.140625 7.734375,-7.125 7.65625,-7.125 7.25,-7.125 6.4375,-7.171875 5.984375,-7.1875 v 0.765625 c 0.21875,0 0.421875,0.03125 0.609375,0.140625 -0.0625,0.109375 -0.0625,0.140625 -0.125,0.203125 L 5.109375,-4.5625 3.53125,-6.421875 H 4.171875 V -7.1875 C 3.75,-7.171875 2.734375,-7.125 2.21875,-7.125 c -0.515625,0 -1.25,-0.046875 -1.78125,-0.0625 v 0.765625 h 1.140625 l 2.53125,2.984375 -2.125,2.375 C 1.71875,-0.765625 1.125,-0.765625 0.359375,-0.765625 V 0 c 0.5625,-0.03125 1.40625,-0.046875 1.484375,-0.046875 0.40625,0 1.34375,0.03125 1.671875,0.046875 v -0.765625 c -0.296875,0 -0.609375,-0.078125 -0.609375,-0.171875 0,-0.015625 0,-0.03125 0.125,-0.15625 l 1.5625,-1.765625 1.78125,2.09375 H 5.734375 V 0 c 0.421875,-0.015625 1.40625,-0.046875 1.953125,-0.046875 0.515625,0 1.234375,0.03125 1.765625,0.046875 V -0.765625 H 8.3125 Z m 0,0"
           id="path869" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph3-0">
        <path
           style="stroke:none"
           d=""
           id="path872" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph3-1">
        <path
           style="stroke:none"
           d="M 5.71875,-2.0625 H 5.328125 c -0.03125,0.25 -0.140625,0.921875 -0.28125,1.03125 -0.09375,0.0625 -0.96875,0.0625 -1.125,0.0625 h -2.09375 c 1.203125,-1.046875 1.59375,-1.359375 2.265625,-1.890625 0.84375,-0.671875 1.625,-1.375 1.625,-2.453125 0,-1.375 -1.203125,-2.203125 -2.65625,-2.203125 -1.390625,0 -2.34375,0.984375 -2.34375,2.015625 0,0.578125 0.484375,0.640625 0.59375,0.640625 0.28125,0 0.609375,-0.1875 0.609375,-0.59375 0,-0.21875 -0.09375,-0.609375 -0.671875,-0.609375 0.34375,-0.796875 1.125,-1.046875 1.65625,-1.046875 1.125,0 1.71875,0.875 1.71875,1.796875 0,0.984375 -0.703125,1.765625 -1.0625,2.171875 L 0.828125,-0.4375 C 0.71875,-0.34375 0.71875,-0.3125 0.71875,0 H 5.375 Z m 0,0"
           id="path875" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph3-2">
        <path
           style="stroke:none"
           d="m 3.09375,-3.78125 c 0.875,0 1.515625,0.609375 1.515625,1.828125 0,1.390625 -0.8125,1.8125 -1.46875,1.8125 -0.453125,0 -1.453125,-0.125 -1.921875,-0.78125 C 1.75,-0.953125 1.875,-1.328125 1.875,-1.5625 c 0,-0.359375 -0.28125,-0.625 -0.625,-0.625 -0.328125,0 -0.640625,0.1875 -0.640625,0.65625 0,1.0625 1.1875,1.765625 2.546875,1.765625 1.578125,0 2.671875,-1.0625 2.671875,-2.1875 0,-0.890625 -0.734375,-1.78125 -1.96875,-2.03125 1.1875,-0.4375 1.609375,-1.28125 1.609375,-1.96875 0,-0.90625 -1.03125,-1.5625 -2.28125,-1.5625 -1.265625,0 -2.21875,0.609375 -2.21875,1.515625 0,0.375 0.25,0.59375 0.578125,0.59375 0.359375,0 0.578125,-0.265625 0.578125,-0.578125 0,-0.328125 -0.21875,-0.5625 -0.578125,-0.59375 0.40625,-0.484375 1.1875,-0.609375 1.609375,-0.609375 0.5,0 1.21875,0.25 1.21875,1.234375 0,0.46875 -0.15625,0.984375 -0.453125,1.34375 C 3.546875,-4.1875 3.234375,-4.15625 2.65625,-4.125 2.375,-4.09375 2.359375,-4.09375 2.296875,-4.09375 2.28125,-4.09375 2.1875,-4.0625 2.1875,-3.9375 c 0,0.15625 0.09375,0.15625 0.296875,0.15625 z m 0,0"
           id="path878" />
      </symbol>
    </g>
    <clipPath
       id="clip2">
      <path
         d="m 49.503906,53.371094 h 13.53125 v 10.148437 h -13.53125 z m 0,0"
         id="path886" />
    </clipPath>
    <clipPath
       id="clip4">
      <path
         d="M 0.503906,0.371094 H 8 V 9 H 0.503906 Z m 0,0"
         id="path889" />
    </clipPath>
    <clipPath
       id="clip5">
      <path
         d="m 9,2 h 5 v 8.519531 H 9 Z m 0,0"
         id="path892" />
    </clipPath>
    <clipPath
       id="clip3">
      <rect
         x="0"
         y="0"
         width="15"
         height="11"
         id="rect895" />
    </clipPath>
    <g
       id="surface11257"
       clip-path="url(#clip3)">
      <g
         clip-path="url(#clip4)"
         clip-rule="nonzero"
         id="g902">
        <g
           style="fill:#000000;fill-opacity:1"
           id="g900">
          <use
             xlink:href="#glyph0-1"
             x="0.21411499"
             y="7.9702482"
             id="use898"
             width="100%"
             height="100%" />
        </g>
      </g>
      <g
         clip-path="url(#clip5)"
         clip-rule="nonzero"
         id="g908">
        <g
           style="fill:#000000;fill-opacity:1"
           id="g906">
          <use
             xlink:href="#glyph1-1"
             x="8.4816084"
             y="10.397565"
             id="use904"
             width="100%"
             height="100%" />
        </g>
      </g>
    </g>
    <clipPath
       id="clip6">
      <path
         d="M 93.476562,77.050781 H 101 V 86 h -7.523438 z m 0,0"
         id="path911" />
    </clipPath>
    <clipPath
       id="clip7">
      <path
         d="m 102,79 h 5.68359 v 8.875 H 102 Z m 0,0"
         id="path914" />
    </clipPath>
    <clipPath
       id="clip8">
      <path
         d="m 154.36328,51.34375 h 10.14844 v 7.441406 h -10.14844 z m 0,0"
         id="path917" />
    </clipPath>
  </defs>
  <g
     id="surface11253"
     transform="translate(-1.8101946,-2.2945578)">
    <path
       style="fill:none;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="M 189.00132,143.99893 216.0011,216.0022"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path926" />
    <path
       style="fill:none;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="M 125.99989,287.99971 216.0011,216.0022"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path928" />
    <path
       style="fill:none;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="M 297.00046,224.99828 216.0011,216.0022"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path930" />
    <path
       style="fill:#d4dceb;fill-opacity:0.6;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 125.99989,287.99971 90.00121,-71.99751 80.99936,8.99608 -17.99793,63.00143 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path932" />
    <path
       style="fill:#bde6d4;fill-opacity:0.6028;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 135.00174,143.99893 h 53.99958 l 26.99978,72.00327 -90.00121,71.99751 H 89.998252 Z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path934" />
    <path
       style="fill:#fdd4b8;fill-opacity:0.6;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 189.00132,143.99893 h 134.99892 l -26.99978,80.99935 -80.99936,-8.99608 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path936" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 160.67926,208.31685 c 1.7611,1.7611 1.7611,4.60775 0,6.36308 -1.75534,1.76111 -4.60198,1.76111 -6.36308,0 -1.75534,-1.75533 -1.75534,-4.60198 0,-6.36308 1.7611,-1.75533 4.60774,-1.75533 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path938" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 268.68417,172.31521 c 1.75534,1.76111 1.75534,4.60775 0,6.36885 -1.7611,1.75534 -4.60775,1.75534 -6.36885,0 -1.75533,-1.7611 -1.75533,-4.60774 0,-6.36885 1.7611,-1.75533 4.60775,-1.75533 6.36885,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path940" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 232.68253,244.31849 c 1.75534,1.75533 1.75534,4.60774 0,6.36308 -1.75533,1.75533 -4.60774,1.75533 -6.36308,0 -1.7611,-1.75534 -1.7611,-4.60775 0,-6.36308 1.75534,-1.75534 4.60775,-1.75534 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path942" />
    <path
       style="fill:#75cab3;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 169.68111,190.31892 c 1.75533,1.75533 1.75533,4.60774 0,6.36308 -1.75533,1.75533 -4.60775,1.75533 -6.36308,0 -1.75533,-1.75534 -1.75533,-4.60775 0,-6.36308 1.75533,-1.75534 4.60775,-1.75534 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path944" />
    <path
       style="fill:#75cab3;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 151.68318,226.32056 c 1.75533,1.75533 1.75533,4.60197 0,6.36307 -1.76111,1.75534 -4.60775,1.75534 -6.36308,0 -1.76111,-1.7611 -1.76111,-4.60774 0,-6.36307 1.75533,-1.76111 4.60197,-1.76111 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path946" />
    <path
       style="fill:#75cab3;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 142.68132,208.31685 c 1.75534,1.7611 1.75534,4.60775 0,6.36308 -1.75533,1.76111 -4.60774,1.76111 -6.36308,0 -1.75533,-1.75533 -1.75533,-4.60198 0,-6.36308 1.75534,-1.75533 4.60775,-1.75533 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path948" />
    <path
       style="fill:#fd9e73;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 241.68439,172.31521 c 1.75533,1.76111 1.75533,4.60775 0,6.36885 -1.76111,1.75534 -4.60775,1.75534 -6.36886,0 -1.75533,-1.7611 -1.75533,-4.60774 0,-6.36885 1.76111,-1.75533 4.60775,-1.75533 6.36886,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path950" />
    <path
       style="fill:#fd9e73;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 286.68211,154.31728 c 1.75533,1.75533 1.75533,4.60774 0,6.36308 -1.75534,1.7611 -4.60775,1.7611 -6.36309,0 -1.7611,-1.75534 -1.7611,-4.60775 0,-6.36308 1.75534,-1.75534 4.60775,-1.75534 6.36309,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path952" />
    <path
       style="fill:#fd9e73;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 286.68211,190.31892 c 1.75533,1.75533 1.75533,4.60774 0,6.36308 -1.75534,1.75533 -4.60775,1.75533 -6.36309,0 -1.7611,-1.75534 -1.7611,-4.60775 0,-6.36308 1.75534,-1.75534 4.60775,-1.75534 6.36309,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path954" />
    <path
       style="fill:#fd9e73;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 259.68232,190.31892 c 1.75533,1.75533 1.75533,4.60774 0,6.36308 -1.75533,1.75533 -4.60775,1.75533 -6.36308,0 -1.76111,-1.75534 -1.76111,-4.60775 0,-6.36308 1.75533,-1.75534 4.60775,-1.75534 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path956" />
    <path
       style="fill:#fd9e73;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 259.68232,154.31728 c 1.75533,1.75533 1.75533,4.60774 0,6.36308 -1.75533,1.7611 -4.60775,1.7611 -6.36308,0 -1.76111,-1.75534 -1.76111,-4.60775 0,-6.36308 1.75533,-1.75534 4.60775,-1.75534 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path958" />
    <path
       style="fill:#75cab3;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 142.68132,190.31892 c 1.75534,1.75533 1.75534,4.60774 0,6.36308 -1.75533,1.75533 -4.60774,1.75533 -6.36308,0 -1.75533,-1.75534 -1.75533,-4.60775 0,-6.36308 1.75534,-1.75534 4.60775,-1.75534 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path960" />
    <path
       style="fill:#75cab3;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 187.68482,217.3187 c 1.75533,1.75533 1.75533,4.60775 0,6.36308 -1.76111,1.75533 -4.60775,1.75533 -6.36886,0 -1.75533,-1.75533 -1.75533,-4.60775 0,-6.36308 1.76111,-1.75533 4.60775,-1.75533 6.36886,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path962" />
    <path
       style="fill:#9cb0d4;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 205.68275,244.31849 c 1.75533,1.75533 1.75533,4.60774 0,6.36308 -1.75533,1.75533 -4.60775,1.75533 -6.36308,0 -1.76111,-1.75534 -1.76111,-4.60775 0,-6.36308 1.75533,-1.75534 4.60775,-1.75534 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path964" />
    <path
       style="fill:#9cb0d4;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 259.68232,235.31663 c 1.75533,1.76111 1.75533,4.60775 0,6.36308 -1.75533,1.76111 -4.60775,1.76111 -6.36308,0 -1.76111,-1.75533 -1.76111,-4.60197 0,-6.36308 1.75533,-1.75533 4.60775,-1.75533 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path966" />
    <path
       style="fill:#9cb0d4;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 259.68232,253.32034 c 1.75533,1.75533 1.75533,4.60198 0,6.36308 -1.75533,1.75533 -4.60775,1.75533 -6.36308,0 -1.76111,-1.7611 -1.76111,-4.60775 0,-6.36308 1.75533,-1.76111 4.60775,-1.76111 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path968" />
    <path
       style="fill:#9cb0d4;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 223.68068,226.32056 c 1.76111,1.75533 1.76111,4.60197 0,6.36307 -1.75533,1.75534 -4.60775,1.75534 -6.36308,0 -1.75533,-1.7611 -1.75533,-4.60774 0,-6.36307 1.75533,-1.76111 4.60775,-1.76111 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path970" />
    <path
       style="fill:#9cb0d4;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 214.6846,262.31642 c 1.75533,1.76111 1.75533,4.60775 0,6.36308 -1.76111,1.76111 -4.60775,1.76111 -6.36885,0 -1.75534,-1.75533 -1.75534,-4.60197 0,-6.36308 1.7611,-1.75533 4.60774,-1.75533 6.36885,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path972" />
    <g
       clip-path="url(#clip2)"
       clip-rule="nonzero"
       id="g976">
      <use
         xlink:href="#surface11257"
         transform="translate(49,53)"
         id="use974"
         x="0"
         y="0"
         width="100%"
         height="100%" />
    </g>
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 143.99782,211.49839 h 4.18047"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path978" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 151.37715,211.49839 -3.19886,-1.20102 v 2.40204 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path980" />
    <g
       style="fill:#000000;fill-opacity:1"
       id="g984">
      <use
         xlink:href="#glyph2-1"
         x="127.68932"
         y="28.497747"
         id="use982"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#000000;fill-opacity:1"
       id="g988">
      <use
         xlink:href="#glyph3-1"
         x="135.9568"
         y="30.925064"
         id="use986"
         width="100%"
         height="100%" />
    </g>
    <g
       clip-path="url(#clip6)"
       clip-rule="nonzero"
       id="g994">
      <g
         style="fill:#000000;fill-opacity:1"
         id="g992">
        <use
           xlink:href="#glyph2-1"
           x="93.187317"
           y="84.918648"
           id="use990"
           width="100%"
           height="100%" />
      </g>
    </g>
    <g
       clip-path="url(#clip7)"
       clip-rule="nonzero"
       id="g1000">
      <g
         style="fill:#000000;fill-opacity:1"
         id="g998">
        <use
           xlink:href="#glyph3-2"
           x="101.45481"
           y="87.345963"
           id="use996"
           width="100%"
           height="100%" />
      </g>
    </g>
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 142.68132,196.682 8.22812,8.22812"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1002" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 153.1729,207.17357 -1.41466,-3.11225 -1.69759,1.69759 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1004" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 164.4844,197.52502 -2.81777,5.63554"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1006" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 160.23465,206.02452 2.50597,-2.32697 -2.14798,-1.07398 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1008" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 180.23043,219.07403 -13.88676,-4.62507"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1010" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 163.30648,213.43272 2.6561,2.15375 0.76218,-2.27501 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1012" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 150.51103,225.47176 2.81777,-5.63554"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1014" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 154.76078,216.97225 -2.50019,2.32697 2.1422,1.07399 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1016" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 206.99925,247.50003 h 13.18231"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1018" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 223.38043,247.50003 -3.19887,-1.20102 v 2.40203 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1020" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 214.6846,262.31642 8.22235,-8.22812"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1022" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 225.1704,251.82484 -3.10647,1.41466 1.69181,1.69759 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1024" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 252.22793,255.07567 -13.88676,-4.63084"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1026" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 235.30976,249.43436 2.65609,2.15375 0.75641,-2.28078 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1028" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 252.22793,239.92438 -13.88676,4.63085"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1030" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 235.30976,245.5657 3.4125,0.12703 -0.75641,-2.28078 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1032" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 222.51431,233.52666 2.81777,5.63554"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1034" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 226.76406,242.02616 -0.35799,-3.40096 -2.14798,1.07399 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1036" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 258.51017,189.47589 2.82355,-5.64131"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1038" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none"
       d="m 119.02344,27.648438 -1.69141,1.570312 1.44922,0.726562 z m 0,0"
       id="path1040" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 262.75993,180.97639 -2.5002,2.32697 2.1422,1.06821 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1042" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 243.00089,175.50252 h 13.17654"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1044" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 259.38207,175.50252 -3.20464,-1.20101 v 2.39626 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1046" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 258.51017,161.52338 2.82355,5.64131"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1048" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 262.75993,170.02288 -0.358,-3.39518 -2.1422,1.07399 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1050" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 280.31902,160.68036 -8.22812,8.22812"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1052" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 269.82745,171.17193 3.11225,-1.41466 -1.69759,-1.69759 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1054" />
    <path
       style="fill:none;stroke:#000000;stroke-width:0.8;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="M 280.31902,190.31892 272.0909,182.0908"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1056" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:0.8;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 269.82745,179.82734 1.41466,3.11225 1.69759,-1.69759 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1058" />
    <path
       style="fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 232.68253,199.32077 c 1.75534,1.75533 1.75534,4.60197 0,6.36308 -1.75533,1.75533 -4.60774,1.75533 -6.36308,0 -1.7611,-1.76111 -1.7611,-4.60775 0,-6.36308 1.75534,-1.76111 4.60775,-1.76111 6.36308,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1060" />
    <path
       style="fill:none;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-dasharray:1, 4;stroke-opacity:1"
       d="m 233.09827,199.80002 28.80131,-21.60098"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1062" />
    <path
       style="fill:none;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-dasharray:1, 4;stroke-opacity:1"
       d="m 229.50099,207.00035 v 36.00164"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1064" />
    <path
       style="fill:none;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-dasharray:1, 4;stroke-opacity:1"
       d="m 225.03182,203.05662 -63.06494,7.88745"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1066" />
    <g
       clip-path="url(#clip8)"
       clip-rule="nonzero"
       id="g1072">
      <g
         style="fill:#000000;fill-opacity:1"
         id="g1070">
        <use
           xlink:href="#glyph2-2"
           x="154.62744"
           y="58.534748"
           id="use1068"
           width="100%"
           height="100%" />
      </g>
    </g>
    <path
       style="fill:none;stroke:#000000;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
       d="m 314.99839,219.01629 c -9.64278,-3.08915 -27.27694,-8.39557 -44.99772,-12.01594 -13.15922,-2.69074 -23.27548,-3.81669 -30.10626,-4.26708"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1074" />
    <path
       style="fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1"
       d="m 235.89872,202.55428 3.9264,1.67449 0.13858,-2.99677 z m 0,0"
       transform="matrix(0.67651,0,0,0.67651,-58.73628,-94.7839)"
       id="path1076" />
  </g>
</svg>
)\n",
        "---\n",
        "Based on these prototypes, we want to classify a new example. Remember that since we want to learn the encoding function $f_{\\theta}$, this classification must be differentiable, and hence, we need to define a probability distribution across classes. For this, we will make use of the distance function $d_{\\varphi}$: the closer a new example $\\mathbf{x}$ is to a prototype $\\mathbf{v}_c$, the higher the probability for $\\mathbf{x}$ belonging to class $c$. Formally, we can simply use a softmax over the distances of $\\mathbf{x}$ to all class prototypes:\n",
        "\n",
        "$$p(y=c\\vert\\mathbf{x})=\\text{softmax}(-d_{\\varphi}(f_{\\theta}(\\mathbf{x}), \\mathbf{v}_c))=\\frac{\\exp\\left(-d_{\\varphi}(f_{\\theta}(\\mathbf{x}), \\mathbf{v}_c)\\right)}{\\sum_{c'\\in \\mathcal{C}}\\exp\\left(-d_{\\varphi}(f_{\\theta}(\\mathbf{x}), \\mathbf{v}_{c'})\\right)}$$\n",
        "\n",
        "Note that the negative sign is necessary since we want to increase the probability for close-by vectors and have a low probability for distant vectors. We train the network $f_{\\theta}$ based on the cross-entropy error of the training query set examples. Thereby, the gradient flows through both the prototypes $\\mathbf{v}_c$ and the query set encodings $f_{\\theta}(\\mathbf{x})$. For the distance function $d_{\\varphi}$, we can choose any function as long as it is differentiable concerning both of its inputs. The most common function, which we also use here, is the squared euclidean distance, but feel free to add your own suggestions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkiYZl4eNvp1"
      },
      "source": [
        "Define the encoder function $f_{\\theta}$, for our purposes it will be a DenseNet (use torchvision for this).\n",
        "\n",
        "You should use common hyperparameters of 64 initial feature channels, add 32 per block, and use a bottleneck size of 64 (i.e. 2 times the growth rate).\n",
        "We use 4 stages of 6 layers each, which results in overall about 1 million parameters.\n",
        "\n",
        "Note that the torchvision package assumes that the last layer is used for classification and hence calls its output size `num_classes`.\n",
        "\n",
        "However, we can instead just use it as the feature space of ProtoNet and choose an arbitrary dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek9LEowwNvp1"
      },
      "outputs": [],
      "source": [
        "def get_convnet(output_size):\n",
        "    convnet = torchvision.models.DenseNet(  # TODO: Fill this according to the instructions above\n",
        "                                          num_init_features=64,\n",
        "                                          growth_rate=32,\n",
        "                                          bn_size=2,\n",
        "                                          block_config=(6, 6, 6, 6),\n",
        "                                          num_classes=output_size,\n",
        "                                          memory_efficient=False\n",
        "                                          )\n",
        "\n",
        "    feat_dim = convnet.classifier.in_features\n",
        "    convnet.classifier = nn.Linear(feat_dim, output_size)\n",
        "\n",
        "    return convnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFIun-cP-c4d"
      },
      "source": [
        "# Advanced Techniques for Robust Few-Shot Learning\n",
        "\n",
        "We will add two important enhancements to our ProtoNet implementation to boost its robustness and adaptability to new domains:\n",
        "\n",
        "## CORAL Loss for Feature Alignment (1p)\n",
        "\n",
        "When facing domain shifts between training and testing distributions, features can exhibit different statistical properties. The CORAL (CORrelation ALignment) loss aligns the second-order statistics (covariance) between support and query features, encouraging domain-invariant representations.\n",
        "\n",
        "Implementation benefits:\n",
        "- Reduces the impact of domain shift\n",
        "- Improves generalization to new domains\n",
        "- Creates more transferable features\n",
        "\n",
        "Reference: [Deep CORAL: Correlation Alignment for Deep Domain Adaptation](https://arxiv.org/abs/1607.01719)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC5xh64h-eSo"
      },
      "outputs": [],
      "source": [
        "def coral_loss(source, target):\n",
        "    # TODO: Fill this\n",
        "\n",
        "    embedding_dim = source.size(1)\n",
        "    batch_size = source.size(0)\n",
        "\n",
        "    source_mean = torch.mean(source, dim=0, keepdim=True)\n",
        "    source_cov = (source - source_mean).T @ (source - source_mean) / (batch_size - 1)\n",
        "\n",
        "    target_mean = torch.mean(target, dim=0, keepdim=True)\n",
        "    target_cov = (target - target_mean).T @ (target - target_mean) / (batch_size - 1)\n",
        "\n",
        "    diff = source_cov - target_cov\n",
        "    loss = torch.mean(diff ** 2)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaUova3dWy1K"
      },
      "source": [
        "## Auxiliary Discrimination Branch (1p)\n",
        "\n",
        "To enhance feature discrimination, we'll add an auxiliary branch that classifies whether features come from the support or query set. This branch:\n",
        "- Acts as a regularizer for the feature extractor\n",
        "- Encourages the network to learn domain-aware features\n",
        "- Provides additional supervision signals during training\n",
        "\n",
        "You can experiment with the architecture of this branch to find the optimal configuration for your specific few-shot learning task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7v0Lnd_W0MM"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the Auxiliary Discrimination Branch inside the ProtoNet class #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_7NiVxwNvp1"
      },
      "source": [
        "Next, implement ProtoNet.\n",
        "The first step during training is to encode all images in a batch with our network.\n",
        "Next, we calculate the class prototypes from the support set (function `calculate_prototypes`), and classify the query set examples according to the prototypes (function `classify_feats`).\n",
        "Keep in mind that we use the data sampling described before, such that the support and query set are stacked together in the batch.\n",
        "Thus, we use our previously defined function `split_batch` to split them apart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGdho7L8Nvp2"
      },
      "outputs": [],
      "source": [
        "class ProtoNet(nn.Module):\n",
        "    def __init__(self, proto_dim, lr):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            proto_dim - Dimensionality of prototype feature space\n",
        "            lr - Learning rate of Adam optimizer\n",
        "        \"\"\"\n",
        "        super(ProtoNet, self).__init__()\n",
        "        self.proto_dim = proto_dim\n",
        "        self.lr = lr\n",
        "        self.model = get_convnet(output_size=proto_dim)\n",
        "        self.optimizer = optim.AdamW(self.parameters(), lr=lr)\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[140, 180], gamma=0.1)\n",
        "\n",
        "        # --- Auxiliary Branch for Discrimination Loss --- #\n",
        "        # This branch predicts whether a feature comes from the support (0) or query (1) set.\n",
        "        self.aux_classifier = nn.Sequential(\n",
        "            nn.Linear(proto_dim, proto_dim),\n",
        "            nn.BatchNorm1d(proto_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(proto_dim, proto_dim // 2),\n",
        "            nn.BatchNorm1d(proto_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(proto_dim // 2, 2)\n",
        "        )\n",
        "        self.lambda_aux = 0.1\n",
        "        self.lambda_coral = 1\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_prototypes(features, targets):\n",
        "        # TODO: Fill this, remember to average class feature vectors during the calculation of prototypes\n",
        "\n",
        "        prototypes = []\n",
        "        classes = torch.unique(targets)\n",
        "        for c in classes:\n",
        "            mask = targets == c\n",
        "            prot = features[mask].mean(dim=0)\n",
        "            prototypes.append(prot)\n",
        "\n",
        "        prototypes = torch.stack(prototypes, dim=0)\n",
        "        return prototypes, classes\n",
        "\n",
        "    def classify_feats(self, prototypes, classes, feats, targets):\n",
        "        # TODO: Fill this using squared euclidean as your distance.\n",
        "\n",
        "        distances = torch.cdist(feats, prototypes, p=2).pow(2)\n",
        "        logits = -distances\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        class_to_idx = {c.item(): i for i, c in enumerate(classes)}\n",
        "        labels = torch.tensor([class_to_idx[t.item()] for t in targets]).to(preds.device)\n",
        "\n",
        "        acc = (preds == labels).float().mean()\n",
        "\n",
        "        return preds, labels, acc\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        features = self.model(imgs)\n",
        "        return features\n",
        "\n",
        "    def calculate_loss(self, features, targets):\n",
        "        # TODO: Fill this\n",
        "\n",
        "        support_features, query_features, support_targets, query_targets = split_batch(features, targets)\n",
        "\n",
        "        prototypes, classes = self.calculate_prototypes(support_features, support_targets)\n",
        "        preds, labels, acc = self.classify_feats(prototypes, classes, query_features, query_targets)\n",
        "\n",
        "        ce_loss = F.cross_entropy(-torch.cdist(query_features, prototypes, p=2).pow(2), labels)\n",
        "\n",
        "        domain_labels = torch.cat([\n",
        "            torch.zeros(support_features.size(0), dtype=torch.long),\n",
        "            torch.ones(query_features.size(0), dtype=torch.long)\n",
        "        ]).to(features.device)\n",
        "\n",
        "        all_feats = torch.cat([support_features, query_features], dim=0)\n",
        "        domain_logits = self.aux_classifier(all_feats)\n",
        "        aux_loss = F.cross_entropy(domain_logits, domain_labels)\n",
        "\n",
        "        cor_loss = coral_loss(support_features, query_features)\n",
        "\n",
        "        loss = ce_loss + self.lambda_aux * aux_loss + self.lambda_coral * cor_loss\n",
        "\n",
        "        return loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7jeyAMMNvp2"
      },
      "source": [
        "### Training and Validation (2p)\n",
        "\n",
        "We recommend training for about 20 epochs and with a 64-dimensional feature space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRgu5DjBNvp2"
      },
      "outputs": [],
      "source": [
        "def train_model(model_class, train_loader, val_loader, proto_dim, lr, max_epochs=20, **kwargs):\n",
        "    # Initialize the model\n",
        "    model = model_class(proto_dim=proto_dim, lr=lr)\n",
        "    model.to(device)\n",
        "\n",
        "    # Train the model, validate every epoch and keep the best model.\n",
        "    # Todo: Fill this #\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        total_train_acc = 0.0\n",
        "        train_count = 0\n",
        "        for imgs, targets in train_loader:\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "            features = model(imgs)\n",
        "            loss, acc = model.calculate_loss(features, targets)\n",
        "\n",
        "            model.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            model.optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "            total_train_acc += acc\n",
        "            train_count += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / train_count if train_count > 0 else 0\n",
        "        avg_train_acc = total_train_acc / train_count if train_count > 0 else 0\n",
        "        model.scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_acc = 0.0\n",
        "        val_count = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, targets in val_loader:\n",
        "                imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "                features = model(imgs)\n",
        "                _, acc = model.calculate_loss(features, targets)\n",
        "\n",
        "                val_acc += acc\n",
        "                val_count += 1\n",
        "\n",
        "        avg_val_acc = val_acc / val_count if val_count > 0 else 0\n",
        "        print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Train Acc={avg_train_acc:.4f} | Val Acc={avg_val_acc:.4f}\")\n",
        "\n",
        "        if avg_val_acc > best_acc:\n",
        "            best_acc = avg_val_acc\n",
        "            best_state = model.state_dict()\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA68-4T9Nvp2",
        "outputId": "8a26bd3d-262c-477c-de0d-f3418e19bee8",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "protonet_model = train_model(ProtoNet,\n",
        "                             proto_dim=64,\n",
        "                             lr=2e-4,\n",
        "                             train_loader=train_data_loader,\n",
        "                             val_loader=val_data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZ_rpJRNvp9"
      },
      "source": [
        "### Testing (1p)\n",
        "\n",
        "Our goal of meta-learning is to obtain a model that can quickly adapt to a new task, or in this case, new classes to distinguish between. To test this, we will use our trained ProtoNet and adapt it to the 10 test classes. Thereby, we pick $k$ examples per class from which we determine the prototypes and test the classification accuracy on all other examples. This can be seen as using the $k$ examples per class as a support set, and the rest of the dataset as a query set. We iterate through the dataset such that each example has been once included in a support set. The average performance across all support sets tells us how well we can expect ProtoNet to perform when seeing only $k$ examples per class. During training, we used $k=4$. In testing, we will experiment with $k=\\{2,4,8,16,32\\}$ to get a better sense of how $k$ influences the results. We would expect that we achieve higher accuracies the more examples we have in the support set, but we don't know how it scales. Hence, let's first implement a function that executes the testing procedure for a given $k$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azM7WKYhNvp-"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_proto_net(model, dataset, data_feats=None, k_shot=4):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "        model - Pretrained ProtoNet model\n",
        "        dataset - The dataset on which the test should be performed.\n",
        "                  Should be instance of ImageDataset\n",
        "        data_feats - The encoded features of all images in the dataset.\n",
        "                     If None, they will be newly calculated, and returned\n",
        "                     for later usage.\n",
        "        k_shot - Number of examples per class in the support set.\n",
        "        The encoder network remains unchanged across k-shot settings. Hence, we only need to extract the features for all images once.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    num_classes = dataset.targets.unique().shape[0]\n",
        "    exmps_per_class = dataset.targets.shape[0]//num_classes  # We assume uniform example distribution here\n",
        "\n",
        "\n",
        "    if data_feats is None:\n",
        "\n",
        "\n",
        "        # TODO: Extract features and targets #\n",
        "        # Sort by classes, so that we obtain tensors of shape [num_classes, exmps_per_class, ...]\n",
        "        # ... #\n",
        "\n",
        "        loader = data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "        all_features, all_targets = [], []\n",
        "\n",
        "        for imgs, targets in loader:\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "            feats = model(imgs)\n",
        "            all_features.append(feats)\n",
        "            all_targets.append(targets)\n",
        "\n",
        "        img_features = torch.cat(all_features, dim=0)\n",
        "        img_targets = torch.cat(all_targets, dim=0)\n",
        "\n",
        "        sorted_idx = torch.argsort(img_targets)\n",
        "        img_features = img_features[sorted_idx].view(num_classes, exmps_per_class, -1)\n",
        "        img_targets = img_targets[sorted_idx].view(num_classes, exmps_per_class)\n",
        "        unique_classes = torch.sort(img_targets[:, 0]).values\n",
        "\n",
        "    else:\n",
        "        img_features, img_targets = data_feats\n",
        "        unique_classes = torch.sort(img_targets[:, 0]).values\n",
        "\n",
        "    # We iterate through the full dataset in two manners.\n",
        "    # First, to select the k-shot batch.\n",
        "    # Second, the evaluate the model on all other examples\n",
        "\n",
        "    feat_dim = img_features.shape[2]\n",
        "    accuracies = []\n",
        "    for k_idx in range(0, exmps_per_class, k_shot):\n",
        "\n",
        "\n",
        "        # Select support set and calculate prototypes\n",
        "\n",
        "        # TODO: Fill this #\n",
        "\n",
        "        support_feats = img_features[:, k_idx:k_idx + k_shot, :]\n",
        "        prototypes = support_feats.mean(dim=1)\n",
        "\n",
        "        # Evaluate accuracy on the rest of the dataset #\n",
        "        batch_acc = 0.0\n",
        "        num_queries = 0\n",
        "        for e_idx in range(0, exmps_per_class, k_shot):\n",
        "            if k_idx == e_idx:  # Do not evaluate on the support set examples\n",
        "                continue\n",
        "\n",
        "            #  TODO: Fill this #\n",
        "\n",
        "            q_feats = img_features[:, e_idx:e_idx + k_shot, :].reshape(-1, feat_dim)\n",
        "            q_labels = img_targets[:, e_idx:e_idx + k_shot].reshape(-1)\n",
        "\n",
        "            distances = torch.cdist(q_feats, prototypes)\n",
        "            pred_indices = torch.argmin(distances, dim=1)\n",
        "\n",
        "            pred_classes = unique_classes[pred_indices]\n",
        "\n",
        "            batch_acc += (pred_classes == q_labels).float().mean().item()\n",
        "            num_queries += 1\n",
        "\n",
        "        accuracies.append(batch_acc / num_queries)\n",
        "\n",
        "    return (mean(accuracies), stdev(accuracies)), (img_features, img_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZHJvRXCNvp-"
      },
      "source": [
        "Testing ProtoNet is relatively quick if we have processed all images once. Hence, we can do in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyIB2RnONvp-",
        "outputId": "179ebebb-77d6-4cc9-8f85-55ec256f296c"
      },
      "outputs": [],
      "source": [
        "protonet_accuracies = dict()\n",
        "data_feats = None\n",
        "for k in [2, 4, 8, 16, 32]:\n",
        "    protonet_accuracies[k], data_feats = test_proto_net(protonet_model, test_set, data_feats=data_feats, k_shot=k)\n",
        "    print(f\"Accuracy for k={k}: {100.0*protonet_accuracies[k][0]:4.2f}% (+-{100*protonet_accuracies[k][1]:4.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0braQZ3WNvp-"
      },
      "source": [
        "Plot the accuracies over number of examples in the support set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd8eKezlNvp-"
      },
      "outputs": [],
      "source": [
        "def plot_few_shot(acc_dict, name, color=None, ax=None):\n",
        "    sns.set()\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(1,1,figsize=(5,3))\n",
        "    ks = sorted(list(acc_dict.keys()))\n",
        "    mean_accs = [acc_dict[k][0] for k in ks]\n",
        "    std_accs = [acc_dict[k][1] for k in ks]\n",
        "    ax.plot(ks, mean_accs, marker='o', markeredgecolor='k', markersize=6, label=name, color=color)\n",
        "    ax.fill_between(ks, [m-s for m,s in zip(mean_accs, std_accs)], [m+s for m,s in zip(mean_accs, std_accs)], alpha=0.2, color=color)\n",
        "    ax.set_xticks(ks)\n",
        "    ax.set_xlim([ks[0]-1, ks[-1]+1])\n",
        "    ax.set_xlabel(\"Number of shots per class\", weight='bold')\n",
        "    ax.set_ylabel(\"Accuracy\", weight='bold')\n",
        "    if len(ax.get_title()) == 0:\n",
        "        ax.set_title(\"Few-Shot Performance \" + name, weight='bold')\n",
        "    else:\n",
        "        ax.set_title(ax.get_title() + \" and \" + name, weight='bold')\n",
        "    ax.legend()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "_G_ukS73Nvp_",
        "outputId": "47c61cc9-2a77-4280-baa5-b3a7b9d6d61d"
      },
      "outputs": [],
      "source": [
        "ax = plot_few_shot(protonet_accuracies, name=\"ProtoNet\", color=\"C1\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEJK6TIzNvqC"
      },
      "source": [
        "# Domain adaptation in the SVHN (1p)\n",
        "\n",
        "So far, we have evaluated our meta-learning algorithms on the same dataset on which we have trained them. However, meta-learning algorithms are especially interesting when we want to move from one to another dataset. So, what happens if we apply them on a quite different dataset than CIFAR?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaexRB_hNvqC"
      },
      "source": [
        "The Street View House Numbers (SVHN) dataset is a real-world image dataset for house number detection. It is similar to MNIST by having the classes 0 to 9, but is more difficult due to its real-world setting and possible distracting numbers left and right. Let's first load the dataset, and visualize some images to get an impression of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uRcQ6jQNvqC",
        "outputId": "429977ab-4ca2-4243-acf0-301e6eab01a4"
      },
      "outputs": [],
      "source": [
        "SVHN_test_dataset = SVHN(root=DATASET_PATH, split='test', download=True, transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "ZGqbkEUlNvqC",
        "outputId": "646b3892-e04c-46ba-d386-f1a89ecbd603"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples\n",
        "# TODO: Fill this #\n",
        "show_images(SVHN_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9eBEFqwNvqC"
      },
      "source": [
        "Each image is labeled with one class between 0 and 9 representing the main digit in the image. Can our ProtoNet learn to classify the digits from only a few examples? This is what we will test out below. The images have the same size as CIFAR, so that we can use the images without changes.\n",
        "\n",
        "Prepare the dataset, for which we take the first 500 images per class. For this dataset, we use our test functions as before to get an estimated performance for different number of shots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI9LCwxbNvqC"
      },
      "outputs": [],
      "source": [
        "# TODO: Prepare the Dataset in an ImageDataset class, limit number of examples to 500 to reduce test time #\n",
        "def svhn_dataset(dataset, examples_per_class=500):\n",
        "    images = dataset.data.transpose(0, 2, 3, 1)\n",
        "    labels = dataset.labels\n",
        "\n",
        "    selected_indices = []\n",
        "    class_counts = {i: 0 for i in range(10)}\n",
        "\n",
        "    for idx, label in enumerate(labels):\n",
        "        if class_counts[label] < examples_per_class:\n",
        "            selected_indices.append(idx)\n",
        "            class_counts[label] += 1\n",
        "\n",
        "    svhn_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(DATA_MEANS, DATA_STD)\n",
        "    ])\n",
        "\n",
        "    return ImageDataset(\n",
        "        imgs=images[selected_indices],\n",
        "        targets=torch.LongTensor(labels[selected_indices]),\n",
        "        img_transform=svhn_transform\n",
        "    )\n",
        "\n",
        "svhn_test = svhn_dataset(SVHN_test_dataset, examples_per_class=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd0nCeBqNvqC"
      },
      "source": [
        "### Experiments\n",
        "\n",
        "First, we can apply ProtoNet to the SVHN dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9P3aOalNvqC",
        "outputId": "a73f0030-451b-4337-94f5-3b5e9a3a8abe"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill this #\n",
        "protonet_svhn_accuracies = dict()\n",
        "svhn_data_feats = None\n",
        "\n",
        "for k in [2, 4, 8, 16, 32]:\n",
        "    protonet_svhn_accuracies[k], svhn_data_feats = test_proto_net(\n",
        "        protonet_model,\n",
        "        svhn_test,\n",
        "        data_feats=svhn_data_feats,\n",
        "        k_shot=k\n",
        "    )\n",
        "    print(f\"Accuracy for k={k}: {100.0*protonet_svhn_accuracies[k][0]:4.2f}% (+-{100*protonet_svhn_accuracies[k][1]:4.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ86wG7HNvqD"
      },
      "source": [
        "It becomes clear that the results are much lower than the ones on CIFAR, and just slightly above random for $k=2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "R_Q1TbU5NvqD",
        "outputId": "d67691b3-cb8e-465a-fe80-4ce680ba573e"
      },
      "outputs": [],
      "source": [
        "ax = plot_few_shot(protonet_svhn_accuracies, name=\"ProtoNet\", color=\"C1\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUOjIRUMpCdV"
      },
      "source": [
        "Repeat the experiments again by re-training on MNIST and testing on SVHN.\n",
        "What do you expect in terms of performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EePZDMSNpn0_",
        "outputId": "b5233119-8ece-4022-9c8d-e03a793e4fcb"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill this #\n",
        "# Hint: Project MNIST to RGB by repeating 3 times the single grayscale channel #\n",
        "\n",
        "classes = torch.randperm(10)\n",
        "train_classes = classes[:7].tolist()\n",
        "val_classes = classes[7:].tolist()\n",
        "\n",
        "MNIST_MEANS = [0.1307, 0.1307, 0.1307]\n",
        "MNIST_STD = [0.3081, 0.3081, 0.3081]\n",
        "\n",
        "MNIST_train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MNIST_MEANS, MNIST_STD),\n",
        "])\n",
        "\n",
        "\n",
        "MNIST_test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MNIST_MEANS, MNIST_STD),\n",
        "])\n",
        "\n",
        "MNIST_train_set = MNIST(root=DATASET_PATH, train=True,  download=True, transform=transforms.ToTensor())\n",
        "MNIST_test_set = MNIST(root=DATASET_PATH, train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "MNIST_all_images = np.concatenate([MNIST_train_set.data.numpy(), MNIST_test_set.data.numpy()], axis=0)\n",
        "MNIST_all_targets = torch.LongTensor(MNIST_train_set.targets.tolist() + MNIST_test_set.targets.tolist())\n",
        "\n",
        "train_set = dataset_from_labels(MNIST_all_images, MNIST_all_targets, train_classes, img_transform=MNIST_train_transform)\n",
        "val_set = dataset_from_labels(MNIST_all_images, MNIST_all_targets, val_classes, img_transform=MNIST_test_transform)\n",
        "\n",
        "mnist_train_data_loader = data.DataLoader(train_set,\n",
        "                                          batch_sampler=FewShotBatchSampler(train_set.targets,\n",
        "                                                                            include_query=True,\n",
        "                                                                            N_way=N_WAY,\n",
        "                                                                            K_shot=K_SHOT,\n",
        "                                                                            shuffle=True),\n",
        "                                          num_workers=4)\n",
        "mnist_val_data_loader = data.DataLoader(val_set,\n",
        "                                        batch_sampler=FewShotBatchSampler(val_set.targets,\n",
        "                                                                          include_query=True,\n",
        "                                                                          N_way=N_WAY,\n",
        "                                                                          K_shot=K_SHOT,\n",
        "                                                                          shuffle=False,\n",
        "                                                                          shuffle_once=True),\n",
        "                                        num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLGdMIpGTY7i",
        "outputId": "1b3bfa5a-7444-4cf0-c0e2-e5d708c33cff"
      },
      "outputs": [],
      "source": [
        "mnist_protonet_model = train_model(ProtoNet,\n",
        "                                  proto_dim=64,\n",
        "                                  lr=2e-4,\n",
        "                                  train_loader=mnist_train_data_loader,\n",
        "                                  val_loader=mnist_val_data_loader,\n",
        "                                  max_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wI3nHFQTjg5",
        "outputId": "9711c4d4-6d67-44b2-84a6-e7d07f840e1f"
      },
      "outputs": [],
      "source": [
        "mnist_protonet_svhn_accuracies = dict()\n",
        "svhn_data_feats = None\n",
        "\n",
        "for k in [2, 4, 8, 16, 32]:\n",
        "    mnist_protonet_svhn_accuracies[k], svhn_data_feats = test_proto_net(\n",
        "        mnist_protonet_model,\n",
        "        svhn_test,\n",
        "        data_feats=svhn_data_feats,\n",
        "        k_shot=k\n",
        "    )\n",
        "    print(f\"Accuracy for k={k}: {100.0*mnist_protonet_svhn_accuracies[k][0]:4.2f}% (+-{100*mnist_protonet_svhn_accuracies[k][1]:4.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "2nXykHK9TyGf",
        "outputId": "003bee51-6120-4c14-826a-f8938f9165db"
      },
      "outputs": [],
      "source": [
        "ax = plot_few_shot(mnist_protonet_svhn_accuracies, name=\"ProtoNet\", color=\"C1\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
