{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-88vApIVjMm"
      },
      "source": [
        "## Texture vs Shape: introduction\n",
        "(homework #2 for DNN 2024/25)\n",
        "\n",
        "<div style=\"max-width: 60em\">\n",
        "\n",
        "In this homework, we will look at a modern residual convolutional net. While they perform very well on image classification tasks, some problems they commonly have are that:<br>\n",
        "* they rely too much on small-scale features (textures) rather than large-scale ones (shape). This often generalizes poorly to unseen datasets and is less human-aligned (e.g. explanations of why a model chose this class may be less interpretable).\n",
        "* they are very susceptible to adversarial images, i.e. inputs maliciously altered in a way that is imperceptible to humans and shouldn't change the classification, but completely fool the model, making it output high probabilities for unrelated classes.\n",
        "\n",
        "While the two problems are somewhat related, we focus on the first one.\n",
        "You are given a pre-trained model and datasets, as well as the code for standard evaluation and training loops, and for generating adversarial examples.\n",
        "Your tasks (fully detailed in later cells) will be as follows:\n",
        "\n",
        "0. Adapt a pretrained model to the smaller datasets by changing the classification layer.\n",
        "1. Implement transformations (blur, pixelize, noise) that can help us elucidate and perhaps remedy the texture-bias problem.\n",
        "2. Compare how the model handles them and how much they help as augmentations used in fine-tuning.\n",
        "3. Read a paper about AdvProp: a relatively simple method for training models with less texture bias, using adversarial examples; implement and evaluate it.\n",
        "4. Implement and evaluate SparseTopK: another, even simpler method proposed for the same problem: it just zeroes all but the top 20% activations in some layers.\n",
        "\n",
        "Submission requirements:\n",
        "* Please do not modify anything outside of the `### BEGIN SOLUTION ... ### END SOLUTION` blocks (if you feel it might be useful somewhere, ask on slack).\n",
        "* Please make sure to submit your solution as a notebook with saved cell outputs.\n",
        "* Don't use `tqdm.notebook` or `tqdm.auto`, just plain `tqdm`.\n",
        "* It should be possible to execute the notebook from top to bottom when loaded in Colab with a GPU, in a reasonable amount of time.\n",
        "* Plots should either be images embedded in cell outputs (in a way that shows correctly when loading the notebook from file, without executing it), or attached together with the exported .ipynb notebook in a .zip file  (screenshots are admissible, but prefer exporting with e.g. `plt.savefig()`).\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHAFsKjOVjMn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d8bh8NXVjMo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import multiprocessing\n",
        "from collections.abc import Callable, Iterable, Sequence\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL.Image\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models\n",
        "import torch.optim.lr_scheduler\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Dataset, Subset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import v2\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from tqdm import tqdm\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Feel free to add more imports and global settings here, though none are necessary.\n",
        "### END SOLUTION\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "multiprocessing.set_start_method(\"spawn\", force=True)  # Needed when using CUDA.\n",
        "torch.set_num_threads(8)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
        "    torch.cuda.set_device(2)\n",
        "\n",
        "_ = torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YVaHNsUVjMp"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvclczGFVjMq",
        "outputId": "ece1a3d8-e752-4cca-9ae9-17959c7ac2f2"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# 219 MiB\n",
        "if [ ! -d datasets ]; then\n",
        "    gdown https://drive.google.com/uc?id=1Xpl0QQaAuTULtGvpdixTISDkUNMhzBMg \\\n",
        "        && tar -xf datasets.tar.gz \\\n",
        "        && rm datasets.tar.gz\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTAx1buOVjMq"
      },
      "source": [
        "##### MiniImageNet\n",
        "MiniImageNet is a downscaled subset of ISVLRC ImageNet-1k<sup>[1]</sup> with only 10 classes (RGB, irregular sizes up to 256x256).\n",
        "Train/val is a uniformly random split, the parts have 2000/100 images per class, respectively.\n",
        "\n",
        "We will use the *train* part for fine-tuning and *val* for validation and various evaluation.\n",
        "\n",
        "[1] https://www.kaggle.com/competitions/imagenet-object-localization-challenge/overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "a3GVZe0ZVjMr",
        "outputId": "e2338e53-6543-401b-fc93-871da6511fc2"
      },
      "outputs": [],
      "source": [
        "MINI_IMAGENET_PATH = Path(\"datasets/miniImageNet\")\n",
        "SELECTED_CLASSES = ImageFolder(MINI_IMAGENET_PATH / \"train\").classes\n",
        "assert SELECTED_CLASSES == ImageFolder(MINI_IMAGENET_PATH / \"val\").classes\n",
        "\n",
        "\n",
        "def example() -> None:  # Note: we scope examples to avoid polluting the global namespace.\n",
        "    print(SELECTED_CLASSES)\n",
        "    dataset = ImageFolder(MINI_IMAGENET_PATH / \"val\")\n",
        "    img, label = dataset[505]\n",
        "    display(img)\n",
        "    print(label, dataset.classes[label])\n",
        "\n",
        "\n",
        "example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTm-HPSsVjMr"
      },
      "source": [
        "##### eval_transform, display_image_tensor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "hQbAAI8_VjMr",
        "outputId": "d1a31e83-7666-4c04-ef1e-1ee8521c65a3"
      },
      "outputs": [],
      "source": [
        "def get_eval_transform() -> v2.Compose:\n",
        "    \"\"\"\n",
        "    The default transform used on all ImageNet images before passing to a model.\n",
        "\n",
        "    (Typically papers end up with 224x224 images, we make them smaller for speed.)\n",
        "    \"\"\"\n",
        "    return v2.Compose(\n",
        "        [\n",
        "            v2.PILToTensor(),\n",
        "            v2.Resize(size=128, antialias=True),\n",
        "            v2.CenterCrop(size=(112, 112)),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def normalize(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Normalize an image tensor of shape (*B, C, H, W) from 0..1 values.\"\"\"\n",
        "    std = torch.tensor(IMAGENET_DEFAULT_STD, device=x.device, dtype=x.dtype).reshape(3, 1, 1)\n",
        "    mean = torch.tensor(IMAGENET_DEFAULT_MEAN, device=x.device, dtype=x.dtype).reshape(3, 1, 1)\n",
        "    return (x - mean) / std\n",
        "\n",
        "\n",
        "def unnormalize(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Revert an image tensor of shape (*B, C, H, W) to 0..1 values.\"\"\"\n",
        "    std = torch.tensor(IMAGENET_DEFAULT_STD, device=x.device, dtype=x.dtype).reshape(3, 1, 1)\n",
        "    mean = torch.tensor(IMAGENET_DEFAULT_MEAN, device=x.device, dtype=x.dtype).reshape(3, 1, 1)\n",
        "    return x * std + mean\n",
        "\n",
        "\n",
        "def display_image_tensor(x: torch.Tensor) -> None:\n",
        "    \"\"\"Display a normalized image tensor of shape (C, H, W) as a PIL Image.\"\"\"\n",
        "    pil_image: PIL.Image.Image = v2.ToPILImage()(unnormalize(x))\n",
        "    display(pil_image)\n",
        "\n",
        "\n",
        "def ceildiv(a: int, b: int) -> int:\n",
        "    \"\"\"Return ceil(a /b).\"\"\"\n",
        "    return -(-a // b)\n",
        "\n",
        "\n",
        "def example() -> None:\n",
        "    dataset = ImageFolder(MINI_IMAGENET_PATH / \"val\", transform=get_eval_transform())\n",
        "    img, label = dataset[505]\n",
        "    display_image_tensor(img)\n",
        "    print(f\"{img.shape=}, {img.dtype=}, min..max={img.min().item():.4g}..{img.max().item():.4g}\")\n",
        "    print(f\"{label=} ({dataset.classes[label]})\")\n",
        "\n",
        "\n",
        "example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYAEXI1NVjMs"
      },
      "source": [
        "##### alt_test_dataset\n",
        "This is a dataset with the same classes and some of the same images, but restylized or otherwise altered to mislead models reliant on textures.\n",
        "We will use this dataset to test if a training method results in a model that is less reliant on textures.\n",
        "Note that some alterations are pretty heavy, so the images look very different and it is difficult to achieve more than ~40-50% accuracy.\n",
        "\n",
        "RGB, 224x224, 280 images per class.\n",
        "\n",
        "(The dataset was created from https://github.com/bethgelab/model-vs-human/releases/tag/v0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "k9XzVd4YVjMs",
        "outputId": "732f8963-34c5-4a78-ccbe-c308d5720c97"
      },
      "outputs": [],
      "source": [
        "ALT_TEST_DATASET_PATHS = sorted(Path(\"datasets/testSets\").iterdir())\n",
        "alt_test_dataset = ConcatDataset([ImageFolder(p, transform=get_eval_transform()) for p in ALT_TEST_DATASET_PATHS])\n",
        "\n",
        "\n",
        "def example() -> None:\n",
        "    for p in ALT_TEST_DATASET_PATHS:\n",
        "        dataset = ImageFolder(p, transform=get_eval_transform())\n",
        "        assert dataset.classes == SELECTED_CLASSES\n",
        "        print(p.name, len(dataset), end=\", \")\n",
        "        # display_image_tensor(dataset[int(len(dataset) * 0.53)][0])\n",
        "    print(\"total\", len(alt_test_dataset))\n",
        "\n",
        "    img, _label = alt_test_dataset[445]\n",
        "    display_image_tensor(img)\n",
        "\n",
        "\n",
        "example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65--EWuMVjMt"
      },
      "source": [
        "## Evaluate() and get_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISRCuhl_VjMt"
      },
      "source": [
        "Here's an implementation of an evaluation loop, to be used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZM0U758VjMt"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(\n",
        "    dataset: Dataset,\n",
        "    shuffle: bool = False,\n",
        "    batch_size: int = 256,\n",
        "    num_workers: int = 2,\n",
        "    pin_memory: bool = True,\n",
        "    prefetch_factor: int = 2,\n",
        "    persistent_workers: bool = True,\n",
        "    timeout: float = 30.0,\n",
        "    **kwargs: Any,\n",
        ") -> DataLoader:\n",
        "    \"\"\"DataLoader constructor with different defaults.\"\"\"\n",
        "    if num_workers == 0:\n",
        "        pin_memory = False\n",
        "        persistent_workers = False\n",
        "        prefetch_factor = None\n",
        "        timeout = 0.0\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        shuffle=shuffle,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        prefetch_factor=prefetch_factor,\n",
        "        persistent_workers=persistent_workers,\n",
        "        timeout=timeout,\n",
        "        **kwargs,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-yOAvFFVjMt"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: Dataset | DataLoader,\n",
        "    description: str = \"eval\",\n",
        "    device: str = DEVICE,\n",
        "    quiet: bool = False,\n",
        "    use_workers: bool = True,\n",
        ") -> float:\n",
        "    \"\"\"Evaluate a model on a dataset or dataloader, returning the accuracy (0..1).\"\"\"\n",
        "    model = model.to(device).eval()\n",
        "    if not isinstance(dataloader, DataLoader):  # Create from dataset.\n",
        "        if use_workers:\n",
        "            dataloader = get_dataloader(dataloader, persistent_workers=False)\n",
        "        else:\n",
        "            dataloader = get_dataloader(dataloader, num_workers=0)\n",
        "\n",
        "    accuracy, n_done, n_correct = 0, 0, 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=description, disable=quiet, delay=0.5)\n",
        "    with torch.no_grad(), progress_bar:\n",
        "        for image_batch, label_batch in progress_bar:\n",
        "            logits_batch = model(image_batch.to(device))\n",
        "            predictions = logits_batch.argmax(dim=1)\n",
        "\n",
        "            n_done += len(label_batch)\n",
        "            n_correct += (predictions == label_batch.to(device)).sum().item()\n",
        "            accuracy = n_correct / n_done if n_done else 0\n",
        "            progress_bar.set_postfix({\"accuracy\": f\"{accuracy * 100:.1f} %\"})\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MS2efVRVjMu"
      },
      "source": [
        "## Task 0: Model, remapping the classification layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhqiEZdpVjMu"
      },
      "source": [
        "We will use a ready-made model pretrained on ImageNet-1k.<br>\n",
        "Since they all output logits for 1000 classes (`IMAGENET_CATEGORIES`),<br>\n",
        "we need to alter the classification layer to output logits for the 10 MiniImageNet classes instead (`SELECTED_CLASSES`).<br>\n",
        "Note that several ImageNet-1k classes can map to the same MiniImageNet class.<br>\n",
        "\n",
        "Your task is to complete the implementation of `remap_output_layer()` below,<br>\n",
        "so that the model works without any fine-tuning/training of the last layer.<br>\n",
        "In `example_remapping()` below (which evaluates the model on the val dataset),<br>\n",
        "you should achieve at least 70% accuracy and finish in under 35 seconds on Colab CPU (~15s on GPU).<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfc55A_eVjMu",
        "outputId": "cf7debf2-3846-43e8-9a25-94a3e8481f43"
      },
      "outputs": [],
      "source": [
        "SELECTED_MODEL = (torchvision.models.efficientnet_b0, torchvision.models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "# 5.3M params, 78% acc@1 on original ImageNet-1k\n",
        "\n",
        "# Other models we considered, but cut for time:\n",
        "# * (torchvision.models.mobilenet_v3_small, torchvision.models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "#     2.5M params, 68% acc@1\n",
        "# * (torchvision.models.resnet18, torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "#     11.7M params, 70% acc@1 on original ImageNet-1k\n",
        "# * (torchvision.models.efficientnet_v2_s, torchvision.models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "#     21.5M params, 84% acc@1\n",
        "\n",
        "IMAGENET_CATEGORIES = SELECTED_MODEL[1].meta[\"categories\"]\n",
        "assert IMAGENET_CATEGORIES == torchvision.models.ResNet18_Weights.IMAGENET1K_V1.meta[\"categories\"]\n",
        "print(len(IMAGENET_CATEGORIES), \", \".join(IMAGENET_CATEGORIES[:7] + [\"...\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bsnmKgcVjMu"
      },
      "outputs": [],
      "source": [
        "# Some technical details, feel free to ignore.\n",
        "\n",
        "\n",
        "def _load_imagenet_id_to_class_id_map(classes: Sequence[str] = SELECTED_CLASSES) -> dict[int, int]:\n",
        "    \"\"\"\n",
        "    Return a map from imagenet ids 0..999 to selected class ids: 0..len(classes)-1.\n",
        "\n",
        "    Classes should be a subset of the following 16 class names:\n",
        "        airplane,bear,bicycle,bird,boat,bottle,car,cat,chair,clock,dog,elephant,keyboard,knife,oven,truck\n",
        "    \"\"\"\n",
        "    imagenet_id_to_name = {i: name for i, name in enumerate(IMAGENET_CATEGORIES)}\n",
        "    class_to_idx = {name: i for i, name in enumerate(classes)}\n",
        "    with open(\"datasets/imagenet_name_to_synset_id.json\") as f:\n",
        "        name_to_synset = json.load(f)\n",
        "    with open(\"datasets/synset_id_to_class.json\") as f:\n",
        "        synset_to_class = json.load(f)\n",
        "    # Not all synsets defined in synset_id_to_class.json are present in ImageNet-1k\n",
        "    # (they also don't appear in our dataset 10-class-ImageNet).\n",
        "    imagenet_name_to_class_name = {\n",
        "        name: synset_to_class[synset] for name, synset in name_to_synset.items() if synset in synset_to_class\n",
        "    }\n",
        "\n",
        "    result = dict[int, int]()\n",
        "    for imagenet_id, imagenet_name in imagenet_id_to_name.items():\n",
        "        # Not all imagenet names are present in 10-class-ImageNet, obviously.\n",
        "        if imagenet_name in imagenet_name_to_class_name:\n",
        "            class_name = imagenet_name_to_class_name[imagenet_name]\n",
        "            # Not all class names defined for 16-class-ImageNet are present in our dataset 10-class-ImageNet.\n",
        "            if class_name in class_to_idx:\n",
        "                result[imagenet_id] = class_to_idx[class_name]\n",
        "\n",
        "    for k, v in result.items():\n",
        "        assert 0 <= k < len(IMAGENET_CATEGORIES) and 0 <= v < len(classes)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def _reset_model_weights(model: torch.nn.Module) -> None:\n",
        "    \"\"\"Reinitialize the model weights randomly. We probably won't use it.\"\"\"\n",
        "    # Some models define reset_parameters(), but unfortunately not all.\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, torch.nn.Conv2d):\n",
        "            torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
        "            if m.bias is not None:\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, torch.nn.BatchNorm2d | torch.nn.GroupNorm):\n",
        "            torch.nn.init.ones_(m.weight)\n",
        "            torch.nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, torch.nn.Linear):\n",
        "            init_range = 1.0 / np.sqrt(m.out_features)\n",
        "            torch.nn.init.uniform_(m.weight, -init_range, init_range)  # EfficientNet v1\n",
        "            # torch.nn.init.normal_(m.weight, 0, 0.01)  # MobileNet v3\n",
        "            torch.nn.init.zeros_(m.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUA7iebFVjMv"
      },
      "outputs": [],
      "source": [
        "def remap_output_layer(layer: torch.nn.Linear, id_map: dict[int, int]) -> torch.nn.Module:\n",
        "    \"\"\"Return a new final classification logit layer where the i-th class becomes id_map[i] instead.\"\"\"\n",
        "    assert layer.out_features == len(IMAGENET_CATEGORIES)\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    new_out_features = len(set(id_map.values()))\n",
        "\n",
        "    new_layer = torch.nn.Linear(layer.in_features, new_out_features)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        old_weights = layer.weight\n",
        "        new_weights = torch.zeros(new_out_features, layer.in_features)\n",
        "\n",
        "        old_bias = layer.bias\n",
        "        new_bias = torch.zeros(new_out_features)\n",
        "\n",
        "        for old_class, new_class in id_map.items():\n",
        "            new_weights[new_class] += old_weights[old_class]\n",
        "            new_bias[new_class] += old_bias[old_class]\n",
        "\n",
        "        new_weights /= len(id_map)\n",
        "        new_bias /= len(id_map)\n",
        "        new_layer.weight.copy_(new_weights)\n",
        "        new_layer.bias.copy_(new_bias)\n",
        "\n",
        "    return new_layer\n",
        "\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNnEl3mPVjMv",
        "outputId": "400dd7c7-6a9b-4ca6-9756-07a29acda93c"
      },
      "outputs": [],
      "source": [
        "def get_model(\n",
        "    eval: bool = False, reset: bool = False, selected_classes: Sequence[str] = SELECTED_CLASSES\n",
        ") -> torch.nn.Module:\n",
        "    model_fn, model_weights = SELECTED_MODEL\n",
        "    model = model_fn(weights=model_weights)\n",
        "\n",
        "    # Find and replace the last Linear layer.\n",
        "    id_map = _load_imagenet_id_to_class_id_map(selected_classes)\n",
        "    last_linear_layer = None\n",
        "    for parent_module in model.modules():\n",
        "        for name, layer in parent_module.named_children():\n",
        "            if isinstance(layer, torch.nn.Linear):\n",
        "                last_linear_layer = (parent_module, name, layer)\n",
        "    assert last_linear_layer is not None\n",
        "    parent_module, name, layer = last_linear_layer\n",
        "\n",
        "    setattr(parent_module, name, remap_output_layer(layer, id_map))\n",
        "\n",
        "    if reset:\n",
        "        _reset_model_weights(model)\n",
        "\n",
        "    return model.eval() if eval else model.train()\n",
        "\n",
        "\n",
        "get_model(eval=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIAMpEPzVjMv",
        "outputId": "4dd91dee-3190-4fdb-da7a-ddaa950d5101"
      },
      "outputs": [],
      "source": [
        "def example_remapping() -> None:\n",
        "    dataset = ImageFolder(MINI_IMAGENET_PATH / \"val\", transform=get_eval_transform())\n",
        "    model = get_model(eval=True)\n",
        "    evaluate(model, dataset)\n",
        "\n",
        "\n",
        "example_remapping()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fHG-saQVjMv"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQEc2PJDVjMv"
      },
      "source": [
        "Here's an implementation of a training loop that we will use for this notebook.\n",
        "You should not need to alter hyperparameters, use the defaults of `Trainer.__init__()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbEP-eQtVjMv"
      },
      "outputs": [],
      "source": [
        "BATCH_NORM_TYPES = (\n",
        "    torch.nn.BatchNorm1d\n",
        "    | torch.nn.BatchNorm2d\n",
        "    | torch.nn.BatchNorm3d\n",
        "    | torch.nn.SyncBatchNorm\n",
        "    | torch.nn.LazyBatchNorm1d\n",
        "    | torch.nn.LazyBatchNorm2d\n",
        "    | torch.nn.LazyBatchNorm3d\n",
        ")\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        lr: float = 2e-4,\n",
        "        weight_decay: float = 3e-5,\n",
        "        batch_norm_momentum: float | None = 0.002,\n",
        "        n_epochs: int = 10,\n",
        "        device: str = DEVICE,\n",
        "        extra_augmentation: v2.Transform | None = None,\n",
        "        use_workers: bool = True,\n",
        "    ):\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.n_epochs = n_epochs\n",
        "        self.device = device\n",
        "        self.batch_norm_momentum = batch_norm_momentum\n",
        "        if extra_augmentation is not None:\n",
        "            self.extra_augmentation = v2.RandomApply([extra_augmentation], p=0.5)\n",
        "        else:\n",
        "            self.extra_augmentation = v2.GaussianBlur(kernel_size=9, sigma=(0.1, 9.0))\n",
        "        self.num_workers = 2 if use_workers else 0\n",
        "\n",
        "    def get_train_transform(self) -> v2.Transform:\n",
        "        return v2.Compose(\n",
        "            [\n",
        "                v2.PILToTensor(),\n",
        "                v2.RandomResizedCrop(size=(112, 112), antialias=True),\n",
        "                v2.RandomHorizontalFlip(p=0.5),\n",
        "                v2.AutoAugment(interpolation=v2.InterpolationMode.BILINEAR),\n",
        "                v2.ToDtype(torch.float32, scale=True),\n",
        "                v2.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
        "                self.extra_augmentation,\n",
        "                v2.RandomErasing(p=0.2),\n",
        "                v2.ToPureTensor(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def get_train_dataloader(self) -> DataLoader:\n",
        "        transform = self.get_train_transform()\n",
        "        train_dataset = ImageFolder(MINI_IMAGENET_PATH / \"train\", transform=transform)\n",
        "        return get_dataloader(train_dataset, shuffle=True, num_workers=self.num_workers)\n",
        "\n",
        "    def get_eval_dataloaders(self) -> dict[str, DataLoader]:\n",
        "        transform = get_eval_transform()\n",
        "        val_dataset = ImageFolder(MINI_IMAGENET_PATH / \"val\", transform=transform)\n",
        "        return {\n",
        "            \"val\": get_dataloader(val_dataset, num_workers=self.num_workers),\n",
        "            \"alt\": get_dataloader(alt_test_dataset, num_workers=self.num_workers),\n",
        "        }\n",
        "\n",
        "    def get_optimizer_and_scheduler(\n",
        "        self, parameters: Iterable[torch.nn.Parameter]\n",
        "    ) -> tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LRScheduler]:\n",
        "        optimizer = torch.optim.AdamW(parameters, lr=self.lr, weight_decay=self.weight_decay, fused=True)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.97)\n",
        "        return optimizer, lr_scheduler\n",
        "\n",
        "    def do_evaluations(self, model: torch.nn.Module, eval_dataloaders: dict[str, DataLoader]) -> dict[str, float]:\n",
        "        result = dict[str, float]()\n",
        "        for k, d in eval_dataloaders.items():\n",
        "            result[k] = evaluate(model, d, description=k, quiet=True)\n",
        "            print(f\"{k}={result[k] * 100:.1f} %\", end=\"\\t\", flush=True)\n",
        "        print()\n",
        "        return result\n",
        "\n",
        "    def train(self, model: torch.nn.Module) -> dict[str, list[float]]:\n",
        "        model = model.to(self.device)\n",
        "\n",
        "        if self.batch_norm_momentum is not None:\n",
        "            # Default torch.nn.BatchNorm2D.momentum is 0.1, but it's often too high.\n",
        "            for m in model.modules():\n",
        "                if isinstance(m, BATCH_NORM_TYPES):\n",
        "                    m.momentum = self.batch_norm_momentum\n",
        "\n",
        "        train_dataloader = self.get_train_dataloader()\n",
        "        eval_dataloaders = self.get_eval_dataloaders()\n",
        "        optimizer, lr_scheduler = self.get_optimizer_and_scheduler(model.parameters())\n",
        "\n",
        "        results = {k: [] for k in eval_dataloaders.keys()}\n",
        "        for k, v in self.do_evaluations(model, eval_dataloaders).items():\n",
        "            results[k].append(v)\n",
        "\n",
        "        for epoch in range(1, self.n_epochs + 1):\n",
        "            self.train_epoch(model, train_dataloader, optimizer, epoch)\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            for k, v in self.do_evaluations(model, eval_dataloaders).items():\n",
        "                results[k].append(v)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def train_epoch(\n",
        "        self, model: torch.nn.Module, dataloader: DataLoader, optimizer: torch.optim.Optimizer, epoch: int\n",
        "    ) -> None:\n",
        "        model.train()\n",
        "        n_correct = 0\n",
        "        n_total = 0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Train epoch {epoch:>3}\")\n",
        "        for image_batch, label_batch in progress_bar:\n",
        "            image_batch, label_batch = image_batch.to(self.device), label_batch.to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits_batch = model(image_batch)\n",
        "            loss = torch.nn.CrossEntropyLoss()(logits_batch, label_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                n_correct += (logits_batch.argmax(dim=1) == label_batch).sum().item()\n",
        "                n_total += len(label_batch)\n",
        "            accuracy = n_correct / n_total\n",
        "            progress_bar.set_postfix({\"train-acc\": f\"{accuracy * 100:.1f} %\", \"lr\": optimizer.param_groups[0][\"lr\"]})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0gA43WnVjMw"
      },
      "source": [
        "## Task 1: Implementing augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMjr3aYFVjMw"
      },
      "source": [
        "In this task, you are asked to implement a few transformations that can help in assessing whether a model relies more on texture or shape information.\n",
        "\n",
        "Consider using functions from [torch.nn.functional](https://pytorch.org/docs/stable/nn.functional.html), in particular [vision](https://pytorch.org/docs/stable/nn.functional.html#vision-functions), [convolution](https://pytorch.org/docs/stable/nn.functional.html#convolution-functions), and [pooling](https://pytorch.org/docs/stable/nn.functional.html#pooling-functions) functions (or corresponding modules in `torch.nn`).\n",
        "\n",
        "In all cases, the transform should take a normalized image of shape `(*B, C, H, W)` (where `*B` represents any number of extra dimensions, possibly none) and dtype `float32`, and it should output the same.\n",
        "Do not change the tensor's device (it will be CPU).\n",
        "\n",
        "Use the following functions to check and benchmark your transforms. Each benchmark should take <20s (on Colab CPU, the GPU is not used here even if available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "QAYtbex2VjMw",
        "outputId": "06bbda78-cf8b-4e8e-8a54-1aa79c688c4a"
      },
      "outputs": [],
      "source": [
        "def example_transformed(transform: Callable[[torch.Tensor], torch.Tensor], image_id: int = 510, seed: int = 42) -> None:\n",
        "    torch.manual_seed(seed)\n",
        "    dataset = ImageFolder(MINI_IMAGENET_PATH / \"val\", transform=get_eval_transform())\n",
        "    img, label = dataset[image_id]\n",
        "    with torch.no_grad():\n",
        "        img = transform(img)\n",
        "    display_image_tensor(img)\n",
        "\n",
        "\n",
        "def benchmark_transform(transform: v2.Transform) -> None:\n",
        "    dataset = ImageFolder(MINI_IMAGENET_PATH / \"train\", transform=transform)\n",
        "    dataset = Subset(dataset, range(2500))\n",
        "    dataloader = get_dataloader(dataset, batch_size=256, num_workers=0)\n",
        "    for _image_batch, _label_batch in tqdm(dataloader):\n",
        "        pass\n",
        "\n",
        "\n",
        "example_transformed(v2.GaussianBlur(kernel_size=25, sigma=2.0), 513)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZwnwqO0VjMw"
      },
      "source": [
        "#### 1a: Blur\n",
        "Implement a transformation that blurs the image (any method that makes the image visually smoother in all directions is fine).<br>\n",
        "The transform should be parameterized by strength (from roughly 1 meaning no/minimal blurring to 100 meaning the image becomes unrecognizable to humans).<br>\n",
        "For task 1a specifically, do not use any library beyond `torch.*` and python built-ins (in particular, do not use `torchvision`'s `GaussianBlur`).<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "wBpJKSWkVjMw",
        "outputId": "8343e81a-ee48-4b99-a356-0ba58b1b069d"
      },
      "outputs": [],
      "source": [
        "def blur(x: torch.Tensor, strength: int = 10) -> torch.Tensor:\n",
        "    assert 1 <= strength <= 100, f\"Expected 1 ≤ strength ≤ 100, got {strength}.\"\n",
        "    *B, C, H, W = x.shape\n",
        "\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    kernel_size = max(3, (strength // 20) * 2 + 3)\n",
        "\n",
        "    sigma = strength / 10.0\n",
        "\n",
        "    x_coords = torch.arange(kernel_size).float() - kernel_size // 2\n",
        "    y_coords = torch.arange(kernel_size).float() - kernel_size // 2\n",
        "\n",
        "    x_grid, y_grid = torch.meshgrid(x_coords, y_coords, indexing='ij')\n",
        "\n",
        "    kernel = torch.exp(-(x_grid ** 2 + y_grid ** 2) / (2 * sigma ** 2))\n",
        "    kernel /= kernel.sum()\n",
        "    kernel = kernel.expand(C, 1, kernel_size, kernel_size)\n",
        "\n",
        "    padding = (kernel_size - 1) // 2\n",
        "\n",
        "    x = F.conv2d(x, kernel, padding=padding, groups=C)\n",
        "\n",
        "    x = x.view(*B, C, H, W)\n",
        "\n",
        "    ### END SOLUTION\n",
        "    return x\n",
        "\n",
        "\n",
        "example_transformed(blur, 507)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc0f2NsmVjMw",
        "outputId": "934d1338-9295-4e22-fd89-246ebb41f6e5"
      },
      "outputs": [],
      "source": [
        "benchmark_transform(v2.Compose([get_eval_transform(), v2.Lambda(blur)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXGHB05PVjMw"
      },
      "source": [
        "#### 1b: Pixelize\n",
        "Implement a transformation that averages blocks of `block_height x block_width` pixels, as in the example below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfny1yYjVjMw"
      },
      "source": [
        "<img height=160 src=\"data:image/png;base64,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\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "0QKYtl6rVjMx",
        "outputId": "4014aabb-09e4-441a-d890-c8b4a3452e70"
      },
      "outputs": [],
      "source": [
        "def pixelize(x: torch.Tensor, block_height: int = 8, block_width: int = 8) -> torch.Tensor:\n",
        "    assert 1 <= block_height and 1 <= block_width, f\"Expected ≥1, got {block_height=}, {block_width=}.\"\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    *B, C, H, W = x.shape\n",
        "\n",
        "    pad_h = (block_height - H % block_height) % block_height\n",
        "    pad_w = (block_width - W % block_width) % block_width\n",
        "\n",
        "    x = F.pad(x, (0, pad_w, 0, pad_h), mode='replicate')\n",
        "    x = F.avg_pool2d(x, kernel_size=(block_height, block_width))\n",
        "\n",
        "    x = x.repeat_interleave(block_height, dim=-2).repeat_interleave(block_width, dim=-1)\n",
        "\n",
        "    x = x[..., :H, :W]\n",
        "\n",
        "    ### END SOLUTION\n",
        "    return x\n",
        "\n",
        "\n",
        "example_transformed(lambda x: pixelize(x, 25, 12), 510)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QZxFIVgmVjMx",
        "outputId": "833ad255-a296-4e83-a45f-9f638c5974ca"
      },
      "outputs": [],
      "source": [
        "benchmark_transform(v2.Compose([get_eval_transform(), v2.Lambda(pixelize)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irqJNdt1VjMx"
      },
      "source": [
        "#### 1c: Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xwLHIxVVjMx"
      },
      "source": [
        "Implement a transformation that adds noise (same on each channel, up to normalization constants) to an image.<br>\n",
        "Any kind of noise like that is fine, but it should be parameterized by noise strength (from 1 unnoticeable to 100 unrecognizable) and grain size (roughly in pixels), as in the four examples below.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62kPGIEfVjMx"
      },
      "source": [
        "<img height=160 src=\"data:image/png;base64,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\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdrOZoPHVjMx"
      },
      "outputs": [],
      "source": [
        "def noise(x: torch.Tensor, strength: int = 25, grain_size: int = 5) -> torch.Tensor:\n",
        "    assert 1 <= strength <= 100, f\"Expected 1 ≤ strength ≤ 100, got {strength}.\"\n",
        "    assert 1 <= grain_size <= 100, f\"Expected 1 ≤ grain_size ≤ 100, got {grain_size}.\"\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    *B, C, H, W = x.shape\n",
        "\n",
        "    x = x.view(-1, C, H, W)\n",
        "    x = unnormalize(x)\n",
        "\n",
        "    small_h = H // grain_size\n",
        "    small_w = W // grain_size\n",
        "\n",
        "    noise_pattern = torch.randn(1, 1, small_h, small_w)\n",
        "    noise_pattern = F.interpolate(noise_pattern, size=(H, W), mode='bilinear', align_corners=False)\n",
        "    noise_pattern = noise_pattern * (strength / 200.0)\n",
        "\n",
        "    x_noisy = x + noise_pattern\n",
        "    x_noisy = torch.clamp(x_noisy, 0.0, 1.0)\n",
        "    x_noisy = normalize(x_noisy)\n",
        "\n",
        "    x = x_noisy.view(*B, C, H, W)\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "KkvJNB9uVjMy",
        "outputId": "b5eef077-1aee-4d65-fff2-f5115e47a262"
      },
      "outputs": [],
      "source": [
        "example_transformed(noise, 506)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyqMtSBvVjMy",
        "outputId": "2efc2e63-cc5a-45a6-acf1-24bc947881ae"
      },
      "outputs": [],
      "source": [
        "benchmark_transform(v2.Compose([get_eval_transform(), v2.Lambda(noise)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84xWfmaKVjMy"
      },
      "source": [
        "## Task 2: Comparing augmentations\n",
        "\n",
        "Focus on the readability of the presented results.\n",
        "\n",
        "Note: to use transforms defined in a notebook, you will have to call `evaluate()` with `use_workers=False`<br>\n",
        "(this is because functions are pickled by reference, and those defined in a notebook cannot be imported by workers spawned by a dataloader;<br>\n",
        "other workarounds include writing a Python module e.g. with `%%writefile file.py`, or using the `cloudpickle` package, but we prefer to keep it simple here)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqdkRKYkVjMz"
      },
      "source": [
        "#### Side note: lambdas in for loops\n",
        "\n",
        "Since in this task it may be useful to use lots of lambdas (to change default parameters in your transformations),<br>\n",
        "we would like to warn about one potential pitfall here.<br>\n",
        "\n",
        "One 'gotcha' in Python is that 'for' variables (and similar blocks) are not scoped to the loop body,<br>\n",
        "they continue with the value from the last iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJYFw0bHVjMz",
        "outputId": "2a7c52d5-c436-4967-91bd-8d249c699c25"
      },
      "outputs": [],
      "source": [
        "def example() -> None:\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            pass\n",
        "        print(j)\n",
        "\n",
        "\n",
        "example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QknIZ7T4VjMz"
      },
      "source": [
        "This is sometimes useful and mostly benign, but we may overlook using the wrong variable.\n",
        "\n",
        "Another gotcha is that closures in Python (lambdas or def-s using variables from external scopes)<br>\n",
        "are *late-binding*, meaning variables are captured by reference,<br>\n",
        "so especially together with the previous gotcha, we get a probably-not-so-intuitive result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWYiTH1uVjMz",
        "outputId": "c7223363-3ce3-444f-f39c-af0d7f498f45"
      },
      "outputs": [],
      "source": [
        "def example() -> None:\n",
        "    funcs = [(lambda x: x + a) for a in range(3)]\n",
        "    for f in funcs:\n",
        "        print(f(0))\n",
        "\n",
        "\n",
        "example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmKGhH_AVjMz"
      },
      "source": [
        "Use `lambda x, a=a: f(x,a)` or `functools.partial(f, a=a)` to capture by value instead.\n",
        "\n",
        "(A linter like [ruff](https://docs.astral.sh/ruff/) will warn you about both gotchas, by detecting e.g. [unused loop variables](https://docs.astral.sh/ruff/rules/unused-loop-control-variable/) and [loop variables used in closures](https://docs.astral.sh/ruff/rules/function-uses-loop-variable/), unfortunately there is no easy way to use them in Colab)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGBMdgVpVjMz"
      },
      "source": [
        "#### 2a: On vanilla model\n",
        "\n",
        "Compare how `get_model()` performs on inputs from the `val` dataset, when transformed with your transformations (one at a time) or with `v2.GaussianBlur`, using different transform parameters.\n",
        "(Use each transformation after `get_eval_transform()`, as in the benchmarks above.)\n",
        "\n",
        "Plot the results. The total time taken by computations for this subtask must not exceed 10 minutes (on Colab GPU)<br>\n",
        "(use fewer than 100 calls to evaluate(); if running on CPU, expect less than 40 minutes).<br>\n",
        "Discuss the results in 2-4 sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-ML33a8VjMz"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdIQcYcmFkNu"
      },
      "outputs": [],
      "source": [
        "def evaluate_custom_blur(\n",
        "    model: torch.nn.Module,\n",
        "    strengths: list[int] = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        ") -> dict[int, float]:\n",
        "\n",
        "    results = {}\n",
        "    for strength in strengths:\n",
        "        transform = v2.Compose([get_eval_transform(), v2.Lambda(lambda x: blur(x, strength=strength))])\n",
        "        dataset = ImageFolder(MINI_IMAGENET_PATH / \"val\", transform=transform)\n",
        "        acc = evaluate(model, dataset, use_workers=False)\n",
        "        results[strength] = acc\n",
        "\n",
        "    return results\n",
        "\n",
        "def evaluate_custom_pixelize(\n",
        "    model: torch.nn.Module,\n",
        "    block_heights: list[int] = [2, 4, 8, 16],\n",
        "    block_widths: list[int] = [2, 4, 8, 16]\n",
        ") -> dict[tuple[int, int], float]:\n",
        "\n",
        "    results = {}\n",
        "    for block_height in block_heights:\n",
        "        for block_width in block_widths:\n",
        "            transform = v2.Compose([get_eval_transform(), v2.Lambda(lambda x: pixelize(x, block_height, block_width))])\n",
        "            dataset = ImageFolder(MINI_IMAGENET_PATH / \"val\", transform=transform)\n",
        "            acc = evaluate(model, dataset, use_workers=False)\n",
        "            results[(block_height, block_width)] = acc\n",
        "\n",
        "    return results\n",
        "\n",
        "def evaluate_custom_noise(\n",
        "    model: torch.nn.Module,\n",
        "    strengths: list[int] = [10, 30, 50, 70, 90],\n",
        "    grain_sizes: list[int] = [5, 10, 15, 30, 60, 90]\n",
        ") -> dict[tuple[int, int], float]:\n",
        "\n",
        "    results = {}\n",
        "    for strength in strengths:\n",
        "        for grain_size in grain_sizes:\n",
        "            transform = v2.Compose([get_eval_transform(), v2.Lambda(lambda x: noise(x, strength, grain_size))])\n",
        "            dataset = ImageFolder(MINI_IMAGENET_PATH / \"val\", transform=transform)\n",
        "            acc = evaluate(model, dataset, use_workers=False)\n",
        "            results[(strength, grain_size)] = acc\n",
        "\n",
        "    return results\n",
        "\n",
        "def evaluate_gaussian_blur(\n",
        "    model: torch.nn.Module,\n",
        "    kernel_sizes: list[int] = [3, 5, 7, 9],\n",
        "    sigmas: list[float] = [0.5, 1.0, 2.0, 5.0, 10.0]\n",
        ") -> dict[tuple[int, float], float]:\n",
        "\n",
        "    results = {}\n",
        "    for kernel_size in kernel_sizes:\n",
        "        for sigma in sigmas:\n",
        "            transform = v2.Compose([get_eval_transform(), v2.GaussianBlur(kernel_size=kernel_size, sigma=sigma)])\n",
        "            dataset = ImageFolder(MINI_IMAGENET_PATH / \"val\", transform=transform)\n",
        "            acc = evaluate(model, dataset, use_workers=True)\n",
        "            results[(kernel_size, sigma)] = acc\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rQ64MwfiZuo",
        "outputId": "c60de309-1f7c-4f9a-fd1a-8f923e5a0eec"
      },
      "outputs": [],
      "source": [
        "model = get_model(eval=True)\n",
        "custom_blur_results = evaluate_custom_blur(model)\n",
        "custom_pixelize_results = evaluate_custom_pixelize(model)\n",
        "custom_noise_results = evaluate_custom_noise(model)\n",
        "gaussian_blur_results = evaluate_gaussian_blur(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "yxJL1CKx1cYF",
        "outputId": "7c66626a-4725-458d-bfe1-d43859efd6a2"
      },
      "outputs": [],
      "source": [
        "strengths = list(custom_blur_results.keys())\n",
        "accs = list(custom_blur_results.values())\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(strengths, accs, marker='o')\n",
        "plt.title('Custom Blur')\n",
        "plt.xlabel('Strength')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "fn5LWLMa1zRT",
        "outputId": "76e090a6-a42a-40e3-bf0b-1347b2147ce1"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(\n",
        "    [(bh, bw, acc) for (bh, bw), acc in custom_pixelize_results.items()],\n",
        "    columns=['Block Height', 'Block Width', 'Accuracy']\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "for bh in df['Block Height'].unique():\n",
        "    subset = df[df['Block Height'] == bh]\n",
        "    plt.plot(subset['Block Width'], subset['Accuracy'], marker='o', label=f'Height={bh}')\n",
        "\n",
        "plt.title('Custom Pixelize')\n",
        "plt.xlabel('Block Width')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "yGQWH_4JiKQf",
        "outputId": "708bc015-e6ce-4e00-a781-967f44d061de"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(\n",
        "    [(s, g, acc) for (s, g), acc in custom_noise_results.items()],\n",
        "    columns=['Strength', 'Grain Size', 'Accuracy']\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "for g_size in df['Grain Size'].unique():\n",
        "    subset = df[df['Grain Size'] == g_size]\n",
        "    plt.plot(subset['Strength'], subset['Accuracy'], marker='o', label=f'Grain={g_size}')\n",
        "plt.title('Custom Noise')\n",
        "plt.xlabel('Strength')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Y_eaux3O38bw",
        "outputId": "15cb6563-97e7-4f3d-941e-2dd827702486"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(\n",
        "    [(k, s, acc) for (k, s), acc in gaussian_blur_results.items()],\n",
        "    columns=['Kernel Size', 'Sigma', 'Accuracy']\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "for k_size in df['Kernel Size'].unique():\n",
        "    subset = df[df['Kernel Size'] == k_size]\n",
        "    plt.plot(subset['Sigma'], subset['Accuracy'], marker='o', label=f'Kernel={k_size}')\n",
        "plt.title('Gaussian Blur')\n",
        "plt.xlabel('Sigma')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFebNEXSvGx9"
      },
      "source": [
        "1. **Custom Blur Evaluation:** As the blur strength increases, the images lose clarity\n",
        "and fine details, making them progressively unrecognizable, which leads to a significant drop in the model's accuracy.\n",
        "\n",
        "2. **Custom Pixelization Evaluation:**  Starting with small block sizes like 2x2, where multiple pixels are averaged into one, the model's accuracy is already small (~44%), and as the block sizes increase, the image loses even more granularity and recognizable details, leading to even lower accuracy.\n",
        "\n",
        "3. **Custom Noise Evaluation:** For custom noise with low grain sizes, accuracy drops significantly as the noise strength increases, but for higher grain sizes, the drop in accuracy becomes less pronounced since the noise resembles larger patterns that may still preserve some structural information in the image.\n",
        "\n",
        "4. **Gaussian Blur Evaluation:** In Gaussian blur, as the kernel size increases, the accuracy decreases for the same sigma values, since larger kernels apply more extensive blurring, further reducing image clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U65manPkVjMz"
      },
      "outputs": [],
      "source": [
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67PHCjPuVjM0"
      },
      "source": [
        "#### 2b: Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxZ_Tg0bVjM0"
      },
      "source": [
        "Consider the following set of transformations: `{v2.Identity(), noise(strength=25, grain_size=3), v2.GaussianBlur(kernel_size=7, sigma=5)}`.\n",
        "\n",
        "Compare how models fine-tuned for 3 epochs with each augmentation from that set<br>\n",
        "(one at a time, using `Trainer(extra_augmentation=..., n_epochs=3)`, with `use_workers=False` if `noise` is used)<br>\n",
        "performs on val with each augmentation from that set, and on the alt dataset with zero augmentations.<br>\n",
        "We don't expect computations to take more than 20 minutes in total, for this subtask (on Colab GPU; CPU is too slow for fine-tuning, ~20min per epoch).\n",
        "\n",
        "Present the `3 x 4` results in a table. Discuss the results in 1-2 sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF_47HghVjM0"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Og0z1W3M2q",
        "outputId": "e0378088-50da-4b52-ff61-108003e14afb"
      },
      "outputs": [],
      "source": [
        "augmentations = {\n",
        "    \"Identity\": v2.Identity(),\n",
        "    \"Noise\": v2.Lambda(lambda x: noise(x, strength=25, grain_size=3)),\n",
        "    \"GaussianBlur\": v2.GaussianBlur(kernel_size=7, sigma=5)\n",
        "}\n",
        "\n",
        "results = {train_aug: {val_aug: 0 for val_aug in augmentations.keys()} for train_aug in augmentations.keys()}\n",
        "alt_results = {train_aug: 0 for train_aug in augmentations.keys()}\n",
        "\n",
        "\n",
        "for train_aug_name, train_aug in augmentations.items():\n",
        "    print(f\"Training with {train_aug_name} augmentation\")\n",
        "    trainer = Trainer(extra_augmentation=train_aug, n_epochs=3, use_workers=(train_aug_name != \"Noise\"))\n",
        "    model = get_model(eval=True)\n",
        "\n",
        "    trainer.train(model)\n",
        "\n",
        "    for val_aug_name, val_aug in augmentations.items():\n",
        "        print(f\"Evaluating with {val_aug_name} augmentation\")\n",
        "        transform = v2.Compose([get_eval_transform(), val_aug])\n",
        "        dataset = ImageFolder(MINI_IMAGENET_PATH / \"val\", transform=transform)\n",
        "        acc = evaluate(model, dataset, use_workers=(val_aug_name != \"Noise\"))\n",
        "        results[train_aug_name][val_aug_name] = acc\n",
        "\n",
        "    print(\"Evaluating with no augmentation\")\n",
        "    dataset = alt_test_dataset\n",
        "    acc = evaluate(model, dataset, use_workers=(train_aug_name != \"Noise\"))\n",
        "    alt_results[train_aug_name] = acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "bzuxqn8beXRg",
        "outputId": "8fb2bcd1-8afe-43a0-bde6-36c0b59f2c83"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results).transpose()\n",
        "results_df['Alt'] = results_df.index.map(alt_results)\n",
        "\n",
        "results_df = results_df.rename_axis(\"Train Augmentation\").reset_index()\n",
        "\n",
        "columns_order = ['Train Augmentation', 'Identity', 'Noise', 'GaussianBlur', 'Alt']\n",
        "results_df = results_df[columns_order]\n",
        "\n",
        "styled_df = results_df \\\n",
        "    .style.format(precision=3, thousands=\".\", decimal=\",\") \\\n",
        "    .set_properties(**{'text-align': 'center'}) \\\n",
        "    .set_table_styles([{\n",
        "        'selector': 'th',\n",
        "        'props': [('text-align', 'center')]\n",
        "    }]) \\\n",
        "    .hide(axis=\"index\")\n",
        "\n",
        "styled_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weKABqUAx_RV"
      },
      "source": [
        "Models fine-tuned with each specific augmentation achieve high accuracy on the Identity dataset and perform well on their corresponding augmented datasets, demonstrating effective adaptation to each transformation. The results could potentially improve with further fine-tuning over more epochs. However, accuracy remains low on the Alt dataset, indicating that it contains more complex or diverse augmentations that the models were not adequately fine-tuned to handle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTMTbFX0VjM0"
      },
      "outputs": [],
      "source": [
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C_QvjawVjM0"
      },
      "source": [
        "## Adversarial image modification with PGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JqZW6yBVjM0"
      },
      "source": [
        "One method to improve robustness, other than augmentations, is to train on adversarial examples: images that are altered with small perturbations to fool the model.\n",
        "Here's a typical implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "sToI7IwwVjM0",
        "outputId": "04a76c02-3959-4cf6-e483-88150fde47c4"
      },
      "outputs": [],
      "source": [
        "def adversarialPGD(\n",
        "    model: torch.nn.Module,\n",
        "    x: torch.Tensor,\n",
        "    y: torch.Tensor,\n",
        "    epsilon: float = 4 / 255,\n",
        "    alpha: float = 2 / 255,\n",
        "    n_iters: int = 1,\n",
        "    random_init: bool = False,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Find an input xᵃᵈᵛ within x±ε pushing `model(xᵃᵈᵛ)` away from y (maximizing loss, unless alpha is negative).\n",
        "\n",
        "    Uses 'Projective Gradient Descent' with learning rate `alpha` and `n_iters` iterations, as introduced in:\n",
        "    Madry et al., \"Towards Deep Learning Models Resistant to Adversarial Attacks\", ICRL 2018\n",
        "    https://openreview.net/forum?id=rJzIBfZAb\n",
        "\n",
        "    Assumes x is 0..1-valued and keeps it in these bounds (i.e., it expects unnormalized images).\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    x = x.clone().detach()\n",
        "    with torch.no_grad():\n",
        "        x_min = torch.clamp(x - epsilon, min=0)\n",
        "        x_max = torch.clamp(x + epsilon, max=1)\n",
        "\n",
        "        if random_init:\n",
        "            x = x + torch.empty_like(x).uniform_(-epsilon, epsilon)\n",
        "            x.clamp_(x_min, x_max)\n",
        "\n",
        "    for _ in range(n_iters):\n",
        "        x.requires_grad = True\n",
        "        logits = model(x)\n",
        "        loss = torch.nn.CrossEntropyLoss()(logits, y)\n",
        "        with torch.no_grad():\n",
        "            x_grad = torch.autograd.grad(loss, x)[0]\n",
        "            # We could also:\n",
        "            #   loss.backward()\n",
        "            #   x_grad = x.grad.detach()\n",
        "            # but then we'd nedd to temporarily set requires_grad=False on all model parameters.\n",
        "            x = x.detach() + alpha * x_grad.sign()\n",
        "            x.clamp_(x_min, x_max)\n",
        "            x = x.detach()\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "    return x.requires_grad_(False)\n",
        "\n",
        "\n",
        "def example(device=\"cuda\") -> None:\n",
        "    model = get_model(eval=True).to(device)\n",
        "\n",
        "    dataset = ImageFolder(MINI_IMAGENET_PATH / \"val\", transform=get_eval_transform())\n",
        "    img, label = dataset[510]\n",
        "    display_image_tensor(img)\n",
        "    print(f\"{label=} ({dataset.classes[label]})\")\n",
        "\n",
        "    img = img.unsqueeze(0).to(device)\n",
        "    label = torch.tensor([label]).to(device)\n",
        "    pred = model(img)[0].argmax().item()\n",
        "    print(f\"Prediction: {pred} ({dataset.classes[pred]})\")\n",
        "\n",
        "    adv_img = normalize(adversarialPGD(model, unnormalize(img), label, epsilon=20 / 255, n_iters=100))\n",
        "    display_image_tensor(adv_img[0])\n",
        "    pred = model(adv_img)[0].argmax().item()\n",
        "    print(f\"Prediction: {pred} ({dataset.classes[pred]})\")\n",
        "\n",
        "\n",
        "example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAVu34rTVjM1"
      },
      "source": [
        "## Task 3: AdvProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nez0J06jVjM1"
      },
      "source": [
        "Training on adversarial examples unfortunately tends to decrease accuracy on plain (unmodified) images a lot.<br>\n",
        "The authors of [Adversarial Examples Improve Image Recognition](https://arxiv.org/abs/1911.09665) hypothesize that<br>\n",
        "this is because adversarial examples (and the model activations they induce) follow different distributions.<br>\n",
        "They propose addressing that by using auxilliary batch-norm-s for the adversarial images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnWmlCQgVjM1"
      },
      "source": [
        "#### 3a: Implementation\n",
        "Read the paper and implement this technique. Note that:\n",
        "* Parameters (weights and biases of the affine transform applied after normalizing) should be the same for plain and adversarial images; only the running mean and variance (the buffers) are distinct.\n",
        "* During evaluation, the plain batch-norms should be used.\n",
        "* Add the two losses from plain and adversarial images using weights that sum up to one, specifically: `0.75 * plain_loss + 0.25 * adversarial_loss`.\n",
        "* You should not need to alter this and other hyperparameters; use the defaults of `Trainer.__init__()` and `adversarialPGD` (they are different than suggested in the paper, but work better in our case).\n",
        "* See `get_model()` for an example of how layers can be replaced. You can assume all batch-norms are instances of `torch.nn.BatchNorm2d` (with `affine=True`, `track_running_stats=True`, and non-null `momentum`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEso7HLGVjM1"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "\n",
        "class DualBatchNorm2d(torch.nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.adv = False\n",
        "\n",
        "        self.clean_bn = torch.nn.BatchNorm2d(\n",
        "            num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats\n",
        "        )\n",
        "\n",
        "        self.adv_bn = torch.nn.BatchNorm2d(\n",
        "            num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats\n",
        "        )\n",
        "\n",
        "        if affine:\n",
        "            self.adv_bn.weight = self.clean_bn.weight\n",
        "            self.adv_bn.bias = self.clean_bn.bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or not self.adv:\n",
        "            return self.clean_bn(x)\n",
        "        else:\n",
        "            return self.adv_bn(x)\n",
        "\n",
        "\n",
        "def replace_bn_with_dualbn(model, momentum=0.002):\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, torch.nn.BatchNorm2d):\n",
        "            dual_bn = DualBatchNorm2d(\n",
        "                module.num_features,\n",
        "                eps=module.eps,\n",
        "                momentum=momentum,\n",
        "                affine=module.affine,\n",
        "                track_running_stats=module.track_running_stats\n",
        "            )\n",
        "\n",
        "            if module.affine:\n",
        "                with torch.no_grad():\n",
        "                    dual_bn.clean_bn.weight.data.copy_(module.weight.data)\n",
        "                    dual_bn.clean_bn.bias.data.copy_(module.bias.data)\n",
        "\n",
        "            if module.track_running_stats:\n",
        "                with torch.no_grad():\n",
        "                    dual_bn.clean_bn.running_mean.data.copy_(module.running_mean.data)\n",
        "                    dual_bn.clean_bn.running_var.data.copy_(module.running_var.data)\n",
        "                    dual_bn.adv_bn.running_mean.data.copy_(module.running_mean.data)\n",
        "                    dual_bn.adv_bn.running_var.data.copy_(module.running_var.data)\n",
        "\n",
        "            setattr(model, name, dual_bn)\n",
        "        else:\n",
        "            replace_bn_with_dualbn(module, momentum)\n",
        "\n",
        "    return model\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7oPNUvhVjM1"
      },
      "outputs": [],
      "source": [
        "class AdvPropTrainer(Trainer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        lr: float = 2e-4,\n",
        "        weight_decay: float = 3e-5,\n",
        "        batch_norm_momentum: float | None = 0.002,\n",
        "        n_epochs: int = 10,\n",
        "        device: str = \"cuda\",\n",
        "        n_iters: int = 1,\n",
        "        epsilon: float = 4 / 255,\n",
        "        alpha: float = 2 / 255,\n",
        "        adv_ratio: float = 0.25,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            lr=lr, weight_decay=weight_decay, batch_norm_momentum=batch_norm_momentum, n_epochs=n_epochs, device=device\n",
        "        )\n",
        "        self.n_iters = n_iters\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.adv_ratio = adv_ratio\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    def train(self, model: torch.nn.Module) -> dict[str, list[float]]:\n",
        "        model = replace_bn_with_dualbn(model, momentum=self.batch_norm_momentum).to(self.device)\n",
        "        return super().train(model)\n",
        "\n",
        "    def train_epoch(\n",
        "        self, model: torch.nn.Module, dataloader: DataLoader, optimizer: torch.optim.Optimizer, epoch: int\n",
        "    ) -> None:\n",
        "        model.train()\n",
        "\n",
        "        correct_clean = 0\n",
        "        correct_adv = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Train epoch {epoch:>3}\")\n",
        "        ce_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for image_batch, label_batch in progress_bar:\n",
        "            image_batch, label_batch = image_batch.to(self.device), label_batch.to(self.device)\n",
        "\n",
        "            x_adv = adversarialPGD(\n",
        "                model, image_batch, label_batch, epsilon=self.epsilon, alpha=self.alpha, n_iters=self.n_iters\n",
        "            )\n",
        "\n",
        "            for module in model.modules():\n",
        "                if isinstance(module, DualBatchNorm2d):\n",
        "                    module.adv = True\n",
        "\n",
        "            logits_adv = model(x_adv)\n",
        "            loss_adv = ce_loss(logits_adv, label_batch)\n",
        "\n",
        "            for module in model.modules():\n",
        "                if isinstance(module, DualBatchNorm2d):\n",
        "                    module.adv = False\n",
        "\n",
        "            logits_clean = model(image_batch)\n",
        "            loss_clean = ce_loss(logits_clean, label_batch)\n",
        "\n",
        "            loss = (1 - self.adv_ratio) * loss_clean + self.adv_ratio * loss_adv\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                correct_clean += (logits_clean.argmax(dim=1) == label_batch).sum().item()\n",
        "                correct_adv += (logits_adv.argmax(dim=1) == label_batch).sum().item()\n",
        "                total_samples += len(label_batch)\n",
        "\n",
        "            clean_accuracy = correct_clean / total_samples\n",
        "            adv_accuracy = correct_adv / total_samples\n",
        "            progress_bar.set_postfix({\n",
        "                \"clean-acc\": f\"{clean_accuracy * 100:.1f}%\",\n",
        "                \"adv-acc\": f\"{adv_accuracy * 100:.1f}%\",\n",
        "                \"lr\": optimizer.param_groups[0][\"lr\"]\n",
        "            })\n",
        "\n",
        "\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWrK4IgoVjM2"
      },
      "source": [
        "#### 3b: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kod580NiVjM2"
      },
      "source": [
        "Compare Trainer and AdvPropTrainer (using default hyperparameters, 10 epochs, in particular).<br>\n",
        "(One training with Trainer / AdvPropTrainer takes roughly 15 / 20 minutes on colab GPU, respectively).<br>\n",
        "Discuss the results in 1-3 sentences (consider proposing what could be tested further, with more time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peqqB4OpVjM2"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8FJBwYKXDWY",
        "outputId": "f9b69f73-1968-481a-ce49-043f310b55c4"
      },
      "outputs": [],
      "source": [
        "model = get_model()\n",
        "trainer = Trainer()\n",
        "results_trainer = trainer.train(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRWNHAlDWo3m",
        "outputId": "706218fe-dd53-4f89-8216-5a3956a5d06b"
      },
      "outputs": [],
      "source": [
        "adv_model = get_model()\n",
        "adv_trainer = AdvPropTrainer()\n",
        "results_adv_trainer = adv_trainer.train(adv_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcdkcmH61COO"
      },
      "source": [
        "AdvPropTrainer achieves higher accuracy on both the validation set and the alt dataset, consistent with the paper. The improvement on the alt dataset is more pronounced because AdvPropTrainer calculates the loss using both clean and adversarially perturbed images. This dual loss computation allows the model to better generalize and maintain robustness against adversarial perturbations, resulting in enhanced performance on datasets that include such variations.\n",
        "\n",
        "Future work could include testing the model against a variety of adversarial attack types and experimenting with different hyperparameter settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okndNXgGVjM2"
      },
      "outputs": [],
      "source": [
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyJ9DSyLVjM2"
      },
      "source": [
        "## Task 4: SparseTopK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2P3QqZ8VjM2"
      },
      "source": [
        "Another technique to improve robustness against style and pattern changes was proposed in\n",
        "[Emergence of Shape Bias in CNNs through Activation Sparsity](https://openreview.net/forum?id=QzcZb3fWmW)\n",
        "(you do not need to read the paper).\n",
        "The idea is simple: in between some layers, enforce activation sparsity by zeroing out all but the top say 20% activations (by absolute value).\n",
        "The hope is that the strong activations, which we keep, encode the more generalizable shape information.\n",
        "\n",
        "More formally `SparseTopK`, for a fixed fraction $k$ like $20\\%$, should be a module that for an input $x \\in \\mathbb{R}^{C \\times H \\times W}$ outputs:\n",
        "$$ \\begin{align*}\n",
        "    x_{\\text{out}}[c,h,w] &= x[c,h,w]\\quad &&\\text{ if } |x[c,h,w]| \\geq \\text{top-k-percentile}(x[c,:,:]) \\\\\n",
        "                          &= 0 \\quad &&\\text{ otherwise}\n",
        "\\end{align*} $$\n",
        "\n",
        "Your task is to:\n",
        "* implement such a module below (for inputs of shape `(batch_size, C, H, W)`),\n",
        "* run the example once (for 10 epochs),\n",
        "* shortly discuss the result (one sentence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea_EPS4AVjM2",
        "outputId": "285e3d90-5a78-43b9-fb98-d269e41db445"
      },
      "outputs": [],
      "source": [
        "class SparseTopK(torch.nn.Module):\n",
        "    def __init__(self, k: float):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        assert 0 <= k <= 1, f\"Expected 0 ≤ k ≤ 1, got {k}.\"\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        ### BEGIN SOLUTION\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        flat = x.abs().view(B, C, -1)\n",
        "\n",
        "        if self.k < 1.0:\n",
        "            thresholds = torch.quantile(flat, 1 - self.k, dim=-1, keepdim=True)\n",
        "        else:\n",
        "            thresholds = torch.full((B, C, 1), -float('inf'), device=x.device, dtype=x.dtype)\n",
        "\n",
        "        thresholds = thresholds.expand(B, C, H*W).view(B, C, H, W)\n",
        "\n",
        "        mask = (x.abs() >= thresholds)\n",
        "        x = x * mask\n",
        "\n",
        "        ### END SOLUTION\n",
        "        return x\n",
        "\n",
        "\n",
        "def example_sparse() -> None:\n",
        "    model = get_model()\n",
        "    # The paper tests on ResNet18 and suggests inserting one SparseTopK(0.2) module after the second layer.\n",
        "    # We'll do something similar for our model, EfficientNetB0.\n",
        "    model.features[1][0].block.insert(3, SparseTopK(0.2))\n",
        "\n",
        "    Trainer().train(model)\n",
        "\n",
        "\n",
        "example_sparse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3JPl43RVjM2"
      },
      "source": [
        "`### BEGIN SOLUTION (discussion)`\n",
        "\n",
        "It led to a decrease in validation accuracy to 91.1%, while increasing accuracy on the alt dataset to 54.2% (compared to the get_model() run in ex. 3b). The decrease in validation accuracy resulted from the model discarding some activations that are \"useful\". However, the improvement on the alt dataset suggests that enforcing activation sparsity helps the model focus on more generalizable shape information.\n",
        "\n",
        "`### END SOLUTION`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
